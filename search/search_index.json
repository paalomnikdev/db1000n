{"config":{"indexing":"full","lang":["en","ru"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Quick start \u00b6 Death by 1000 needles \u00b6 On 24th of February Russia has launched a full-blown invasion on Ukrainian territory. We're doing our best to stop it and prevent innocent lives being taken Attention Please check existing issues (both open and closed) before creating new ones. It will save me some time answering duplicated questions and right now time is the most critical resource. Regards. Quickstart guide \u00b6 Attention This tool is responsible only for traffic generation, you may want to use VPN if you want to test geo-blocking For dummies \u00b6 Download an application for your platform: Windows Mac M1 Mac Intel Linux 32bit Linux 64bit Unpack the archive Launch the file inside the archive Done! Important Cloud providers could charge a huge amount of money not only for compute resources but for traffic as well. If you run an app in the cloud please control your billing (only advanced users are affected)! Info You can get warnings from your computer about the file - ignore them (or allow in System Settings). Our software is open source. It can be checked and compiled by you yourself. How to install db1000n \u00b6 There are different ways to install and run db1000n Binary file \u00b6 Download the latest release for your arch/OS. Unpack the archive and run it Docker \u00b6 If you already have installed Docker, just run: docker run --rm -it --pull always ghcr.io/arriven/db1000n Or, if your container is not able to connect to your local VPN: docker run --rm -it --pull always --network host ghcr.io/arriven/db1000n Advanced users \u00b6 See For advanced I still have questions \u00b6 You will find some answers on our FAQ","title":"Quick start"},{"location":"#quick-start","text":"","title":"Quick start"},{"location":"#death-by-1000-needles","text":"On 24th of February Russia has launched a full-blown invasion on Ukrainian territory. We're doing our best to stop it and prevent innocent lives being taken Attention Please check existing issues (both open and closed) before creating new ones. It will save me some time answering duplicated questions and right now time is the most critical resource. Regards.","title":"Death by 1000 needles"},{"location":"#quickstart-guide","text":"Attention This tool is responsible only for traffic generation, you may want to use VPN if you want to test geo-blocking","title":"Quickstart guide"},{"location":"#for-dummies","text":"Download an application for your platform: Windows Mac M1 Mac Intel Linux 32bit Linux 64bit Unpack the archive Launch the file inside the archive Done! Important Cloud providers could charge a huge amount of money not only for compute resources but for traffic as well. If you run an app in the cloud please control your billing (only advanced users are affected)! Info You can get warnings from your computer about the file - ignore them (or allow in System Settings). Our software is open source. It can be checked and compiled by you yourself.","title":"For dummies"},{"location":"#how-to-install-db1000n","text":"There are different ways to install and run db1000n","title":"How to install db1000n"},{"location":"#binary-file","text":"Download the latest release for your arch/OS. Unpack the archive and run it","title":"Binary file"},{"location":"#docker","text":"If you already have installed Docker, just run: docker run --rm -it --pull always ghcr.io/arriven/db1000n Or, if your container is not able to connect to your local VPN: docker run --rm -it --pull always --network host ghcr.io/arriven/db1000n","title":"Docker"},{"location":"#advanced-users","text":"See For advanced","title":"Advanced users"},{"location":"#i-still-have-questions","text":"You will find some answers on our FAQ","title":"I still have questions"},{"location":"faq/","text":"FAQ \u00b6 Where can I find advanced documentation? Here I installed db1000n but it's not working properly. What to do? Create Issue and community will help you with solving a problem I'm not a developer, how can I help to project? Share information about db1000n in social media, with your friends and colleagues Run db1000n on every possible platform (local machine, public clouds, Docker, Kubernetes, etc) Create Issues or Pull Requests if you found any bugs, missed documentation, misspells, etc I'm a developer, how can I help to project? Check Issues to help with important tasks Check our codebase and make PRs Test an app on different platforms and report bugs or feature requests When I run db1000n I see that it generates low amount of traffic. Isn't that bad? it's okay The app is configurable to generate set amount of traffic (controlled by the number of targets, their type, and attack interval for each of them). The main reason it works that way is because there are two main types of ddos: Straightforward load generation (easy to implement, easy to defend from) - as effective as the amount of raw traffic you can generate Actual denial of service that aims to remain as undetected as possible by simulating plausible traffic and hitting concrete vulnerabilities in the target (or targets). This type of ddos doesn't require a lot of traffic and thus is mostly limited by the amount of clients generating this type of load (or rather unique IPs) Should I care about costs if I run an app in public cloud? Yes Cloud providers could charge a huge amount of money not only for compute resources but for traffic as well. If you run an app in the cloud please control your billing Can I leave the site for the night? Yes, you can. I personally leave the browser on overnight and it works fine. How can I make sure that the computer does not go to sleep while the site is running? To do this, you need to install a program which keeps the screen turned off. Instructions for different operating systems below: I have Windows: Caffeinated ( download ) I have Mac OS: Amphetamine ( download ) What are primitive jobs? Primitive jobs rely on generating as much raw traffic as possible. This might exhaust your system. They are also easier to detect and unadvisable to be used in the cloud environment. The app shows low response rate, is it ok? Low response rate alone is not enough to be a problem as it could be an indication that current targets are down but you can try to perform additional checks in case you think the rate is abnormal (trying to access one of the targets via curl/browser, checking network stats via other tools like bmon/Task manager, enabling and inspecting debug logs, etc.)","title":"FAQ"},{"location":"faq/#faq","text":"Where can I find advanced documentation? Here I installed db1000n but it's not working properly. What to do? Create Issue and community will help you with solving a problem I'm not a developer, how can I help to project? Share information about db1000n in social media, with your friends and colleagues Run db1000n on every possible platform (local machine, public clouds, Docker, Kubernetes, etc) Create Issues or Pull Requests if you found any bugs, missed documentation, misspells, etc I'm a developer, how can I help to project? Check Issues to help with important tasks Check our codebase and make PRs Test an app on different platforms and report bugs or feature requests When I run db1000n I see that it generates low amount of traffic. Isn't that bad? it's okay The app is configurable to generate set amount of traffic (controlled by the number of targets, their type, and attack interval for each of them). The main reason it works that way is because there are two main types of ddos: Straightforward load generation (easy to implement, easy to defend from) - as effective as the amount of raw traffic you can generate Actual denial of service that aims to remain as undetected as possible by simulating plausible traffic and hitting concrete vulnerabilities in the target (or targets). This type of ddos doesn't require a lot of traffic and thus is mostly limited by the amount of clients generating this type of load (or rather unique IPs) Should I care about costs if I run an app in public cloud? Yes Cloud providers could charge a huge amount of money not only for compute resources but for traffic as well. If you run an app in the cloud please control your billing Can I leave the site for the night? Yes, you can. I personally leave the browser on overnight and it works fine. How can I make sure that the computer does not go to sleep while the site is running? To do this, you need to install a program which keeps the screen turned off. Instructions for different operating systems below: I have Windows: Caffeinated ( download ) I have Mac OS: Amphetamine ( download ) What are primitive jobs? Primitive jobs rely on generating as much raw traffic as possible. This might exhaust your system. They are also easier to detect and unadvisable to be used in the cloud environment. The app shows low response rate, is it ok? Low response rate alone is not enough to be a problem as it could be an indication that current targets are down but you can try to perform additional checks in case you think the rate is abnormal (trying to access one of the targets via curl/browser, checking network stats via other tools like bmon/Task manager, enabling and inspecting debug logs, etc.)","title":"FAQ"},{"location":"license/","text":"License \u00b6 Copyright (c) 2022 Bohdan Ivashko https://github.com/Arriven Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"license/#license","text":"Copyright (c) 2022 Bohdan Ivashko https://github.com/Arriven Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"over-the-air/","text":"Over the air updates \u00b6 Lots of maintainers run their needles on a bare metal machines. As long as this project is so frequently updated, it might be a good idea to let them update it without the hassle. [x] Enabled automatic time-based version check [x] Enabled application self-restart after it downloaded the update Description \u00b6 Support for the application self-update by downloading the latest release from the official repository. Stay strong, be the first in line! The version should be embedded in the binary during the build, see the build target in the Makefile. Usage \u00b6 Available flags \u00b6 -enable-self-update Enable the application automatic updates on the startup -restart-on-update Allows application to restart upon the successful update ( ignored if auto-update is disabled ) ( default true ) -self-update-check-frequency duration How often to run auto-update checks ( default 24h0m0s ) -skip-update-check-on-start Allows to skip the update check at the startup ( usually set automatically by the previous version ) ( default false ) The default behavior if the self-update enabled: * Check for the update * If update is available - download it * If auto-restart is enabled * Notify the user that a newer version is available * Fork-Exec a new process ( will have a different PID ) , add a flag to skip the version check upon startup * Stop the current process * If auto-restart is disabled - notify user that manual restart is required * If update is NOT available - schedule the next check Examples \u00b6 To update your needle, start it with a flag -enable-self-update ./db1000n -enable-self-update Advanced options \u00b6 Start the needle with the self-update & self-restart $ ./db1000n -enable-self-update 0000 /00/00 00 :00:00 main.go:82: DB1000n [ Version: v0.6.4 ][ PID = 75259 ] 0000 /00/00 00 :00:00 main.go:166: Running a check for a newer version... 0000 /00/00 00 :00:00 main.go:176: Newer version of the application is found [ 0 .7.0 ] 0000 /00/00 00 :00:00 main.go:177: What ' s new: * Added some great improvements * Added some spectacular bugs 0000 /00/00 00 :00:00 main.go:180: Auto restart is enabled, restarting the application to run a new version 0000 /00/00 00 :00:00 restart.go:45: new process has been started successfully [ old_pid = 75259 ,new_pid = 75262 ] # NOTE: Process 75259 exited, Process 75262 has started with a flag to skip version check on the startup 0000 /00/00 00 :00:00 main.go:82: DB1000n [ Version: v0.7.0 ][ PID = 75262 ] 0000 /00/00 00 :00:00 main.go:155: Version update on startup is skipped, next update check is scheduled in 24h0m0s Start the needle with the self-update but do not restart the process upon update ( systemd friendly) $ ./db1000n -enable-self-update -self-update-check-frequency = 5m -restart-on-update = false 0000 /00/00 00 :00:00 main.go:82: DB1000n [ Version: v0.6.4 ][ PID = 75320 ] 0000 /00/00 00 :00:00 main.go:166: Running a check for a newer version... 0000 /00/00 00 :00:00 main.go:176: Newer version of the application is found [ 0 .7.0 ] 0000 /00/00 00 :00:00 main.go:177: What ' s new: * Added some great improvements * Added some spectacular bugs 0000 /00/00 00 :00:00 main.go:191: Auto restart is disabled, restart the application manually to apply changes! References \u00b6 Graceful restart with zero downtime for TCP connection - https://github.com/Scalingo/go-graceful-restart-example Graceful restart with zero downtime for TCP connection (two variants) https://github.com/rcrowley/goagain Graceful restart with zero downtime for TCP connection (alternative) https://grisha.org/blog/2014/06/03/graceful-restart-in-golang","title":"Over the air updates"},{"location":"over-the-air/#over-the-air-updates","text":"Lots of maintainers run their needles on a bare metal machines. As long as this project is so frequently updated, it might be a good idea to let them update it without the hassle. [x] Enabled automatic time-based version check [x] Enabled application self-restart after it downloaded the update","title":"Over the air updates"},{"location":"over-the-air/#description","text":"Support for the application self-update by downloading the latest release from the official repository. Stay strong, be the first in line! The version should be embedded in the binary during the build, see the build target in the Makefile.","title":"Description"},{"location":"over-the-air/#usage","text":"","title":"Usage"},{"location":"over-the-air/#available-flags","text":"-enable-self-update Enable the application automatic updates on the startup -restart-on-update Allows application to restart upon the successful update ( ignored if auto-update is disabled ) ( default true ) -self-update-check-frequency duration How often to run auto-update checks ( default 24h0m0s ) -skip-update-check-on-start Allows to skip the update check at the startup ( usually set automatically by the previous version ) ( default false ) The default behavior if the self-update enabled: * Check for the update * If update is available - download it * If auto-restart is enabled * Notify the user that a newer version is available * Fork-Exec a new process ( will have a different PID ) , add a flag to skip the version check upon startup * Stop the current process * If auto-restart is disabled - notify user that manual restart is required * If update is NOT available - schedule the next check","title":"Available flags"},{"location":"over-the-air/#examples","text":"To update your needle, start it with a flag -enable-self-update ./db1000n -enable-self-update","title":"Examples"},{"location":"over-the-air/#advanced-options","text":"Start the needle with the self-update & self-restart $ ./db1000n -enable-self-update 0000 /00/00 00 :00:00 main.go:82: DB1000n [ Version: v0.6.4 ][ PID = 75259 ] 0000 /00/00 00 :00:00 main.go:166: Running a check for a newer version... 0000 /00/00 00 :00:00 main.go:176: Newer version of the application is found [ 0 .7.0 ] 0000 /00/00 00 :00:00 main.go:177: What ' s new: * Added some great improvements * Added some spectacular bugs 0000 /00/00 00 :00:00 main.go:180: Auto restart is enabled, restarting the application to run a new version 0000 /00/00 00 :00:00 restart.go:45: new process has been started successfully [ old_pid = 75259 ,new_pid = 75262 ] # NOTE: Process 75259 exited, Process 75262 has started with a flag to skip version check on the startup 0000 /00/00 00 :00:00 main.go:82: DB1000n [ Version: v0.7.0 ][ PID = 75262 ] 0000 /00/00 00 :00:00 main.go:155: Version update on startup is skipped, next update check is scheduled in 24h0m0s Start the needle with the self-update but do not restart the process upon update ( systemd friendly) $ ./db1000n -enable-self-update -self-update-check-frequency = 5m -restart-on-update = false 0000 /00/00 00 :00:00 main.go:82: DB1000n [ Version: v0.6.4 ][ PID = 75320 ] 0000 /00/00 00 :00:00 main.go:166: Running a check for a newer version... 0000 /00/00 00 :00:00 main.go:176: Newer version of the application is found [ 0 .7.0 ] 0000 /00/00 00 :00:00 main.go:177: What ' s new: * Added some great improvements * Added some spectacular bugs 0000 /00/00 00 :00:00 main.go:191: Auto restart is disabled, restart the application manually to apply changes!","title":"Advanced options"},{"location":"over-the-air/#references","text":"Graceful restart with zero downtime for TCP connection - https://github.com/Scalingo/go-graceful-restart-example Graceful restart with zero downtime for TCP connection (two variants) https://github.com/rcrowley/goagain Graceful restart with zero downtime for TCP connection (alternative) https://grisha.org/blog/2014/06/03/graceful-restart-in-golang","title":"References"},{"location":"advanced-docs/advanced-and-devs/","text":"Advanced and devs \u00b6 For developers \u00b6 Developed by Arriven . This is a simple distributed load generation client written in go. It is able to fetch simple json config from a local or remote location. The config describes which load generation jobs should be launched in parallel. There are other tools doing that. I do not intend to copy or replace them but rather provide a simple open source alternative so that users have more options. Feel free to use it in your load tests (wink-wink). The software is provided as is under no guarantee. I will update both the repo and this doc as I go during following days (date of writing this is 26th of February 2022, third day of Russian invasion into Ukraine). Go installation \u00b6 Run command in your terminal: go install github.com/Arriven/db1000n@latest ~/go/bin/db1000n Shell installation \u00b6 Run install script directly into the shell (useful for installation through SSH): source < ( curl https://raw.githubusercontent.com/Arriven/db1000n/main/install.sh ) The command above will detect the OS and architecture, download the archive, validate it, and extract db1000n executable into the working directory. You can run it via this command: ./db1000n Docker + OpenVPN \u00b6 How to install docker: https://docs.docker.com/get-docker/ Make sure you've set all available resources to docker: Windows Mac Linux See docker-vpn for instructions on setting it up Kubernetes \u00b6 Here possible ways to deploy into it Helm Chart Manifest Public Clouds \u00b6 See possible ways to deploy into public clouds AWS Azure Digital Ocean Googls Cloud Platform Heroku See also \u00b6 db1000nX100 - a project that automates VPN rotation for db1000n instances that allows you to generate geographically distributed traffic (i.e. to stress test geo-blocking)","title":"Advanced and devs"},{"location":"advanced-docs/advanced-and-devs/#advanced-and-devs","text":"","title":"Advanced and devs"},{"location":"advanced-docs/advanced-and-devs/#for-developers","text":"Developed by Arriven . This is a simple distributed load generation client written in go. It is able to fetch simple json config from a local or remote location. The config describes which load generation jobs should be launched in parallel. There are other tools doing that. I do not intend to copy or replace them but rather provide a simple open source alternative so that users have more options. Feel free to use it in your load tests (wink-wink). The software is provided as is under no guarantee. I will update both the repo and this doc as I go during following days (date of writing this is 26th of February 2022, third day of Russian invasion into Ukraine).","title":"For developers"},{"location":"advanced-docs/advanced-and-devs/#go-installation","text":"Run command in your terminal: go install github.com/Arriven/db1000n@latest ~/go/bin/db1000n","title":"Go installation"},{"location":"advanced-docs/advanced-and-devs/#shell-installation","text":"Run install script directly into the shell (useful for installation through SSH): source < ( curl https://raw.githubusercontent.com/Arriven/db1000n/main/install.sh ) The command above will detect the OS and architecture, download the archive, validate it, and extract db1000n executable into the working directory. You can run it via this command: ./db1000n","title":"Shell installation"},{"location":"advanced-docs/advanced-and-devs/#docker--openvpn","text":"How to install docker: https://docs.docker.com/get-docker/ Make sure you've set all available resources to docker: Windows Mac Linux See docker-vpn for instructions on setting it up","title":"Docker + OpenVPN"},{"location":"advanced-docs/advanced-and-devs/#kubernetes","text":"Here possible ways to deploy into it Helm Chart Manifest","title":"Kubernetes"},{"location":"advanced-docs/advanced-and-devs/#public-clouds","text":"See possible ways to deploy into public clouds AWS Azure Digital Ocean Googls Cloud Platform Heroku","title":"Public Clouds"},{"location":"advanced-docs/advanced-and-devs/#see-also","text":"db1000nX100 - a project that automates VPN rotation for db1000n instances that allows you to generate geographically distributed traffic (i.e. to stress test geo-blocking)","title":"See also"},{"location":"advanced-docs/config-encryption/","text":"Encryption \u00b6 How application uses configuration files according to encryption \u00b6 Encryption is done using age as CLI and golang library for encryption/decryption. Under the hood it uses ChaCha20+Poly1305 as AEAD encryption and store encrypted files with pre-defined header age-encryption.org/v1 When application loads config from any source, at first it check is it encrypted (has header). If it is encrypted then tries to decrypt using keys stored in ENCRYPTION_KEYS env variable and iteratively try every key until successful decryption or skip config. If it wasn't encrypted then return as is. You can encrypt every config file with separate keys and run application specifying all keys at once as a list with separator: & . Separator & was chosen to allow pass base64 strings as keys that can encode random keys (binary values). Encrypt new default config \u00b6 Prepare JSON config \u00b6 Let's take next config for example: { \"jobs\" : [ { \"type\" : \"slow-loris\" , \"args\" : { \"address\" : \"127.0.0.1:53\" , \"ContentLength\" : 1000 , \"DialWorkersCount\" : 1 , \"RampUpInterval\" : 1 , \"SleepInterval\" : 1000 , \"DurationSeconds\" : 1000 , \"Path\" : \"https://example.com\" } } ] } and save it as /home/user/config.json Prepare key for encryption \u00b6 Use some of stored in src/utils/crypto.go.ENCRYPTION_KEYS or generate new one. For example lets use: some long password to encrypt config Encrypt config \u00b6 Using make \u00b6 make DEFAULT_CONFIG = /home/user/config.json encrypt_config Enter passphrase ( leave empty to autogenerate a secure one ) : some long password to encrypt config Confirm passphrase: some long password to encrypt config Saved in file: /tmp/fileMx6JFo Save value as env variable: export DEFAULT_CONFIG_VALUE = 'YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IHNjcnlwdCAwS1pOUXlLM004L1NxcDRwL21CaUl3IDE4Cjc1cEZtcmZZZHJieFRvd0hhM0RVVVMxb2VMY0RmQzBkUkJQMXE2UWdyMUEKLS0tIHlSK1VFMkNOSHovbzRqUlJ3RW5VclphRy9TU0NQSG8vMzJUZ1c4RUozZncKD6zE4MONozWBfQYn9HG31DW100o2oFpn6iACQAvCDyXkgSeuQtRFjPwCIW5q2Dltq7Srkc8b81/ZynC59uqkmDJefGyNPzTk3ilRl6wcLOhCP1TD7YtCtZ/7ZpoGpNMiDD6XhKnOmz10sBSy1SXt54+zFVcuQ1ITRi4E2WmiFRjTa8T+ZMwurW+F+iwOu6+z8/0sKQaG5SrKA74GI9D6iRQnqiPg2Abr97Vq7X2Fjvz2NqFjcB0dD29XijHcLCdXQ1DcI3gx94SdMmmfeU5ub2ArsH/4nA8XlS7YE7BirUihgHD4/KIr52dc+Fst6i7SBH433d/Y3Pmhi89FHY8+sGyPFXNG+SeLLHafcR6bLLGyk0iGa2bZaBqUGovYNojni8KSrLRPXTgCyeNAOS7Gpamwi1Xco7m7nEEmAv9vpEvtOUx83pGBOkgu3oSV0t3jmp+OUvcwMMQ=' Encrypted config file saved to /tmp/fileMx6JFo file. And can be saved and distributed anywhere. Using age \u00b6 age --encrypt -p --output = encrypted_config.json /home/user/config.json Enter passphrase ( leave empty to autogenerate a secure one ) : some long password to encrypt config Confirm passphrase: some long password to encrypt config Converting to base64 \u00b6 cat encrypted_config.json | base64 | tr -d '\\n' YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IHNjcnlwdCAwS1pOUXlLM004L1NxcDRwL21CaUl3IDE4Cjc1cEZtcmZZZHJieFRvd0hhM0RVVVMxb2VMY0RmQzBkUkJQMXE2UWdyMUEKLS0tIHlSK1VFMkNOSHovbzRqUlJ3RW5VclphRy9TU0NQSG8vMzJUZ1c4RUozZncKD6zE4MONozWBfQYn9HG31DW100o2oFpn6iACQAvCDyXkgSeuQtRFjPwCIW5q2Dltq7Srkc8b81/ZynC59uqkmDJefGyNPzTk3ilRl6wcLOhCP1TD7YtCtZ/7ZpoGpNMiDD6XhKnOmz10sBSy1SXt54+zFVcuQ1ITRi4E2WmiFRjTa8T+ZMwurW+F+iwOu6+z8/0sKQaG5SrKA74GI9D6iRQnqiPg2Abr97Vq7X2Fjvz2NqFjcB0dD29XijHcLCdXQ1DcI3gx94SdMmmfeU5ub2ArsH/4nA8XlS7YE7BirUihgHD4/KIr52dc+Fst6i7SBH433d/Y3Pmhi89FHY8+sGyPFXNG+SeLLHafcR6bLLGyk0iGa2bZaBqUGovYNojni8KSrLRPXTgCyeNAOS7Gpamwi1Xco7m7nEEmAv9vpEvtOUx83pGBOkgu3oSV0t3jmp+OUvcwMMQ = Encrypt only part of the config \u00b6 You can encrypt a single job and add it to plaintext config (nothing stopping you from encrypting it further afterwards) via encrypted job entry: { \"jobs\" : [ { \"type\" : \"encrypted\" , \"args\" : { \"format\" : \"json\" , \"data\" : \"YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IHNjcnlwdCB5eCtiMzQ5RWlZRXo4dTNpRE8veHdRIDE4CmYyb2d0YXlnaXptS25sbUJlQUVaUHpRbngwaUdBYUpJRStHbFltdUVNNkUKLS0tIG5oUUVCd041TWJoNWNCQjhvODk4eUFpUldmUFUvaStpanRsdCtWR0RrSVkK2ehc+JYVl+f5VgLKV0mG/J4CQrtHn+FFV5AAcKiLEAjU6MNDaVqBI6Qm9RunLZ51wAA13DLZkPJH39DcsS77H3HmgLpRQ7DMFG2AIDxWysIt2Yi2hVVn9Ogea73twGa8FOpk2kk0Z7NSHCCcpTJd1Db4cwYJiIFaqfBXR+VZtNk3qBgUMStN1CiOyJxvHbnc6tbfeqq042LImKsaLvFzB2y5H/ec9BonHimrP/aZv6dhequs\" } }, { \"type\" : \"tcp\" , \"count\" : 100 , \"args\" : { \"address\" : \"localhost:9090\" , \"body\" : \"more_test\" , \"interval_ms\" : 1000 } } ] } Where args.format represents the encoding format you used for the data before encryption and args.data is a single ecrypted job. To encrypt a single job use same steps as to encrypt the whole config but use a file that contains just the job definition: { \"type\" : \"tcp\" , \"count\" : 100 , \"args\" : { \"address\" : \"localhost:9090\" , \"body\" : \"more_test\" , \"interval_ms\" : 1000 } } Note: each single decryption needs at least 256mb of RAM to set up an encryption key (this is implemented by age as hardening against bruteforce attempts). Due to that encrypting multiple jobs separately may be inefficient and it's advised to use parallel job to group multiple independed jobs into one: { \"type\" : \"parallel\" , \"args\" : { \"jobs\" : [ { \"type\" : \"tcp\" , \"count\" : 100 , \"args\" : { \"address\" : \"localhost:9090\" , \"body\" : \"payload\" , \"interval_ms\" : 1000 } }, { \"type\" : \"tcp\" , \"count\" : 100 , \"args\" : { \"address\" : \"localhost:9091\" , \"body\" : \"payload\" , \"interval_ms\" : 1000 } } ] } } Embedding encrypted config as default backup config into binary \u00b6 Export env variable as it printed: export DEFAULT_CONFIG_VALUE = 'YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IHNjcnlwdCAwS1pOUXlLM004L1NxcDRwL21CaUl3IDE4Cjc1cEZtcmZZZHJieFRvd0hhM0RVVVMxb2VMY0RmQzBkUkJQMXE2UWdyMUEKLS0tIHlSK1VFMkNOSHovbzRqUlJ3RW5VclphRy9TU0NQSG8vMzJUZ1c4RUozZncKD6zE4MONozWBfQYn9HG31DW100o2oFpn6iACQAvCDyXkgSeuQtRFjPwCIW5q2Dltq7Srkc8b81/ZynC59uqkmDJefGyNPzTk3ilRl6wcLOhCP1TD7YtCtZ/7ZpoGpNMiDD6XhKnOmz10sBSy1SXt54+zFVcuQ1ITRi4E2WmiFRjTa8T+ZMwurW+F+iwOu6+z8/0sKQaG5SrKA74GI9D6iRQnqiPg2Abr97Vq7X2Fjvz2NqFjcB0dD29XijHcLCdXQ1DcI3gx94SdMmmfeU5ub2ArsH/4nA8XlS7YE7BirUihgHD4/KIr52dc+Fst6i7SBH433d/Y3Pmhi89FHY8+sGyPFXNG+SeLLHafcR6bLLGyk0iGa2bZaBqUGovYNojni8KSrLRPXTgCyeNAOS7Gpamwi1Xco7m7nEEmAv9vpEvtOUx83pGBOkgu3oSV0t3jmp+OUvcwMMQ=' This base64 value is encrypted config with specified password (hashed with scrypt against brute force). Build new binary with new config: make build_encrypted GOOS = linux GOARCH = amd64 CGO_ENABLED = 0 go build -ldflags = \"-X 'github.com/Arriven/db1000n/src/job/config.DefaultConfig=YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IHNjcnlwdCAwS1pOUXlLM004L1NxcDRwL21CaUl3IDE4Cjc1cEZtcmZZZHJieFRvd0hhM0RVVVMxb2VMY0RmQzBkUkJQMXE2UWdyMUEKLS0tIHlSK1VFMkNOSHovbzRqUlJ3RW5VclphRy9TU0NQSG8vMzJUZ1c4RUozZncKD6zE4MONozWBfQYn9HG31DW100o2oFpn6iACQAvCDyXkgSeuQtRFjPwCIW5q2Dltq7Srkc8b81/ZynC59uqkmDJefGyNPzTk3ilRl6wcLOhCP1TD7YtCtZ/7ZpoGpNMiDD6XhKnOmz10sBSy1SXt54+zFVcuQ1ITRi4E2WmiFRjTa8T+ZMwurW+F+iwOu6+z8/0sKQaG5SrKA74GI9D6iRQnqiPg2Abr97Vq7X2Fjvz2NqFjcB0dD29XijHcLCdXQ1DcI3gx94SdMmmfeU5ub2ArsH/4nA8XlS7YE7BirUihgHD4/KIr52dc+Fst6i7SBH433d/Y3Pmhi89FHY8+sGyPFXNG+SeLLHafcR6bLLGyk0iGa2bZaBqUGovYNojni8KSrLRPXTgCyeNAOS7Gpamwi1Xco7m7nEEmAv9vpEvtOUx83pGBOkgu3oSV0t3jmp+OUvcwMMQ='\" -o main ./main.go Your new binary saved as main with new encrypted and embedded default config. To turn on decryption new config you should pass encryption keys as list of keys separated with & symbol: export ENCRYPTION_KEYS = 'some long password to encrypt config&another key' It will override default encryption keys Embedding encryption keys into binary \u00b6 You can embed keys into binary in same way: export ENCRYPTION_KEYS = 'some long password to encrypt config&another key' make build_encrypted GOOS = linux GOARCH = amd64 CGO_ENABLED = 0 go build -ldflags = \"-X 'github.com/Arriven/db1000n/src/utils.EncryptionKeys=some long password to encrypt config&another key' -X 'github.com/Arriven/db1000n/src/job/config.DefaultConfig=YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IHNjcnlwdCAwS1pOUXlLM004L1NxcDRwL21CaUl3IDE4Cjc1cEZtcmZZZHJieFRvd0hhM0RVVVMxb2VMY0RmQzBkUkJQMXE2UWdyMUEKLS0tIHlSK1VFMkNOSHovbzRqUlJ3RW5VclphRy9TU0NQSG8vMzJUZ1c4RUozZncKD6zE4MONozWBfQYn9HG31DW100o2oFpn6iACQAvCDyXkgSeuQtRFjPwCIW5q2Dltq7Srkc8b81/ZynC59uqkmDJefGyNPzTk3ilRl6wcLOhCP1TD7YtCtZ/7ZpoGpNMiDD6XhKnOmz10sBSy1SXt54+zFVcuQ1ITRi4E2WmiFRjTa8T+ZMwurW+F+iwOu6+z8/0sKQaG5SrKA74GI9D6iRQnqiPg2Abr97Vq7X2Fjvz2NqFjcB0dD29XijHcLCdXQ1DcI3gx94SdMmmfeU5ub2ArsH/4nA8XlS7YE7BirUihgHD4/KIr52dc+Fst6i7SBH433d/Y3Pmhi89FHY8+sGyPFXNG+SeLLHafcR6bLLGyk0iGa2bZaBqUGovYNojni8KSrLRPXTgCyeNAOS7Gpamwi1Xco7m7nEEmAv9vpEvtOUx83pGBOkgu3oSV0t3jmp+OUvcwMMQ='\" -o main ./main.go","title":"Encryption"},{"location":"advanced-docs/config-encryption/#encryption","text":"","title":"Encryption"},{"location":"advanced-docs/config-encryption/#how-application-uses-configuration-files-according-to-encryption","text":"Encryption is done using age as CLI and golang library for encryption/decryption. Under the hood it uses ChaCha20+Poly1305 as AEAD encryption and store encrypted files with pre-defined header age-encryption.org/v1 When application loads config from any source, at first it check is it encrypted (has header). If it is encrypted then tries to decrypt using keys stored in ENCRYPTION_KEYS env variable and iteratively try every key until successful decryption or skip config. If it wasn't encrypted then return as is. You can encrypt every config file with separate keys and run application specifying all keys at once as a list with separator: & . Separator & was chosen to allow pass base64 strings as keys that can encode random keys (binary values).","title":"How application uses configuration files according to encryption"},{"location":"advanced-docs/config-encryption/#encrypt-new-default-config","text":"","title":"Encrypt new default config"},{"location":"advanced-docs/config-encryption/#prepare-json-config","text":"Let's take next config for example: { \"jobs\" : [ { \"type\" : \"slow-loris\" , \"args\" : { \"address\" : \"127.0.0.1:53\" , \"ContentLength\" : 1000 , \"DialWorkersCount\" : 1 , \"RampUpInterval\" : 1 , \"SleepInterval\" : 1000 , \"DurationSeconds\" : 1000 , \"Path\" : \"https://example.com\" } } ] } and save it as /home/user/config.json","title":"Prepare JSON config"},{"location":"advanced-docs/config-encryption/#prepare-key-for-encryption","text":"Use some of stored in src/utils/crypto.go.ENCRYPTION_KEYS or generate new one. For example lets use: some long password to encrypt config","title":"Prepare key for encryption"},{"location":"advanced-docs/config-encryption/#encrypt-config","text":"","title":"Encrypt config"},{"location":"advanced-docs/config-encryption/#using-make","text":"make DEFAULT_CONFIG = /home/user/config.json encrypt_config Enter passphrase ( leave empty to autogenerate a secure one ) : some long password to encrypt config Confirm passphrase: some long password to encrypt config Saved in file: /tmp/fileMx6JFo Save value as env variable: export DEFAULT_CONFIG_VALUE = 'YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IHNjcnlwdCAwS1pOUXlLM004L1NxcDRwL21CaUl3IDE4Cjc1cEZtcmZZZHJieFRvd0hhM0RVVVMxb2VMY0RmQzBkUkJQMXE2UWdyMUEKLS0tIHlSK1VFMkNOSHovbzRqUlJ3RW5VclphRy9TU0NQSG8vMzJUZ1c4RUozZncKD6zE4MONozWBfQYn9HG31DW100o2oFpn6iACQAvCDyXkgSeuQtRFjPwCIW5q2Dltq7Srkc8b81/ZynC59uqkmDJefGyNPzTk3ilRl6wcLOhCP1TD7YtCtZ/7ZpoGpNMiDD6XhKnOmz10sBSy1SXt54+zFVcuQ1ITRi4E2WmiFRjTa8T+ZMwurW+F+iwOu6+z8/0sKQaG5SrKA74GI9D6iRQnqiPg2Abr97Vq7X2Fjvz2NqFjcB0dD29XijHcLCdXQ1DcI3gx94SdMmmfeU5ub2ArsH/4nA8XlS7YE7BirUihgHD4/KIr52dc+Fst6i7SBH433d/Y3Pmhi89FHY8+sGyPFXNG+SeLLHafcR6bLLGyk0iGa2bZaBqUGovYNojni8KSrLRPXTgCyeNAOS7Gpamwi1Xco7m7nEEmAv9vpEvtOUx83pGBOkgu3oSV0t3jmp+OUvcwMMQ=' Encrypted config file saved to /tmp/fileMx6JFo file. And can be saved and distributed anywhere.","title":"Using make"},{"location":"advanced-docs/config-encryption/#using-age","text":"age --encrypt -p --output = encrypted_config.json /home/user/config.json Enter passphrase ( leave empty to autogenerate a secure one ) : some long password to encrypt config Confirm passphrase: some long password to encrypt config","title":"Using age"},{"location":"advanced-docs/config-encryption/#converting-to-base64","text":"cat encrypted_config.json | base64 | tr -d '\\n' YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IHNjcnlwdCAwS1pOUXlLM004L1NxcDRwL21CaUl3IDE4Cjc1cEZtcmZZZHJieFRvd0hhM0RVVVMxb2VMY0RmQzBkUkJQMXE2UWdyMUEKLS0tIHlSK1VFMkNOSHovbzRqUlJ3RW5VclphRy9TU0NQSG8vMzJUZ1c4RUozZncKD6zE4MONozWBfQYn9HG31DW100o2oFpn6iACQAvCDyXkgSeuQtRFjPwCIW5q2Dltq7Srkc8b81/ZynC59uqkmDJefGyNPzTk3ilRl6wcLOhCP1TD7YtCtZ/7ZpoGpNMiDD6XhKnOmz10sBSy1SXt54+zFVcuQ1ITRi4E2WmiFRjTa8T+ZMwurW+F+iwOu6+z8/0sKQaG5SrKA74GI9D6iRQnqiPg2Abr97Vq7X2Fjvz2NqFjcB0dD29XijHcLCdXQ1DcI3gx94SdMmmfeU5ub2ArsH/4nA8XlS7YE7BirUihgHD4/KIr52dc+Fst6i7SBH433d/Y3Pmhi89FHY8+sGyPFXNG+SeLLHafcR6bLLGyk0iGa2bZaBqUGovYNojni8KSrLRPXTgCyeNAOS7Gpamwi1Xco7m7nEEmAv9vpEvtOUx83pGBOkgu3oSV0t3jmp+OUvcwMMQ =","title":"Converting to base64"},{"location":"advanced-docs/config-encryption/#encrypt-only-part-of-the-config","text":"You can encrypt a single job and add it to plaintext config (nothing stopping you from encrypting it further afterwards) via encrypted job entry: { \"jobs\" : [ { \"type\" : \"encrypted\" , \"args\" : { \"format\" : \"json\" , \"data\" : \"YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IHNjcnlwdCB5eCtiMzQ5RWlZRXo4dTNpRE8veHdRIDE4CmYyb2d0YXlnaXptS25sbUJlQUVaUHpRbngwaUdBYUpJRStHbFltdUVNNkUKLS0tIG5oUUVCd041TWJoNWNCQjhvODk4eUFpUldmUFUvaStpanRsdCtWR0RrSVkK2ehc+JYVl+f5VgLKV0mG/J4CQrtHn+FFV5AAcKiLEAjU6MNDaVqBI6Qm9RunLZ51wAA13DLZkPJH39DcsS77H3HmgLpRQ7DMFG2AIDxWysIt2Yi2hVVn9Ogea73twGa8FOpk2kk0Z7NSHCCcpTJd1Db4cwYJiIFaqfBXR+VZtNk3qBgUMStN1CiOyJxvHbnc6tbfeqq042LImKsaLvFzB2y5H/ec9BonHimrP/aZv6dhequs\" } }, { \"type\" : \"tcp\" , \"count\" : 100 , \"args\" : { \"address\" : \"localhost:9090\" , \"body\" : \"more_test\" , \"interval_ms\" : 1000 } } ] } Where args.format represents the encoding format you used for the data before encryption and args.data is a single ecrypted job. To encrypt a single job use same steps as to encrypt the whole config but use a file that contains just the job definition: { \"type\" : \"tcp\" , \"count\" : 100 , \"args\" : { \"address\" : \"localhost:9090\" , \"body\" : \"more_test\" , \"interval_ms\" : 1000 } } Note: each single decryption needs at least 256mb of RAM to set up an encryption key (this is implemented by age as hardening against bruteforce attempts). Due to that encrypting multiple jobs separately may be inefficient and it's advised to use parallel job to group multiple independed jobs into one: { \"type\" : \"parallel\" , \"args\" : { \"jobs\" : [ { \"type\" : \"tcp\" , \"count\" : 100 , \"args\" : { \"address\" : \"localhost:9090\" , \"body\" : \"payload\" , \"interval_ms\" : 1000 } }, { \"type\" : \"tcp\" , \"count\" : 100 , \"args\" : { \"address\" : \"localhost:9091\" , \"body\" : \"payload\" , \"interval_ms\" : 1000 } } ] } }","title":"Encrypt only part of the config"},{"location":"advanced-docs/config-encryption/#embedding-encrypted-config-as-default-backup-config-into-binary","text":"Export env variable as it printed: export DEFAULT_CONFIG_VALUE = 'YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IHNjcnlwdCAwS1pOUXlLM004L1NxcDRwL21CaUl3IDE4Cjc1cEZtcmZZZHJieFRvd0hhM0RVVVMxb2VMY0RmQzBkUkJQMXE2UWdyMUEKLS0tIHlSK1VFMkNOSHovbzRqUlJ3RW5VclphRy9TU0NQSG8vMzJUZ1c4RUozZncKD6zE4MONozWBfQYn9HG31DW100o2oFpn6iACQAvCDyXkgSeuQtRFjPwCIW5q2Dltq7Srkc8b81/ZynC59uqkmDJefGyNPzTk3ilRl6wcLOhCP1TD7YtCtZ/7ZpoGpNMiDD6XhKnOmz10sBSy1SXt54+zFVcuQ1ITRi4E2WmiFRjTa8T+ZMwurW+F+iwOu6+z8/0sKQaG5SrKA74GI9D6iRQnqiPg2Abr97Vq7X2Fjvz2NqFjcB0dD29XijHcLCdXQ1DcI3gx94SdMmmfeU5ub2ArsH/4nA8XlS7YE7BirUihgHD4/KIr52dc+Fst6i7SBH433d/Y3Pmhi89FHY8+sGyPFXNG+SeLLHafcR6bLLGyk0iGa2bZaBqUGovYNojni8KSrLRPXTgCyeNAOS7Gpamwi1Xco7m7nEEmAv9vpEvtOUx83pGBOkgu3oSV0t3jmp+OUvcwMMQ=' This base64 value is encrypted config with specified password (hashed with scrypt against brute force). Build new binary with new config: make build_encrypted GOOS = linux GOARCH = amd64 CGO_ENABLED = 0 go build -ldflags = \"-X 'github.com/Arriven/db1000n/src/job/config.DefaultConfig=YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IHNjcnlwdCAwS1pOUXlLM004L1NxcDRwL21CaUl3IDE4Cjc1cEZtcmZZZHJieFRvd0hhM0RVVVMxb2VMY0RmQzBkUkJQMXE2UWdyMUEKLS0tIHlSK1VFMkNOSHovbzRqUlJ3RW5VclphRy9TU0NQSG8vMzJUZ1c4RUozZncKD6zE4MONozWBfQYn9HG31DW100o2oFpn6iACQAvCDyXkgSeuQtRFjPwCIW5q2Dltq7Srkc8b81/ZynC59uqkmDJefGyNPzTk3ilRl6wcLOhCP1TD7YtCtZ/7ZpoGpNMiDD6XhKnOmz10sBSy1SXt54+zFVcuQ1ITRi4E2WmiFRjTa8T+ZMwurW+F+iwOu6+z8/0sKQaG5SrKA74GI9D6iRQnqiPg2Abr97Vq7X2Fjvz2NqFjcB0dD29XijHcLCdXQ1DcI3gx94SdMmmfeU5ub2ArsH/4nA8XlS7YE7BirUihgHD4/KIr52dc+Fst6i7SBH433d/Y3Pmhi89FHY8+sGyPFXNG+SeLLHafcR6bLLGyk0iGa2bZaBqUGovYNojni8KSrLRPXTgCyeNAOS7Gpamwi1Xco7m7nEEmAv9vpEvtOUx83pGBOkgu3oSV0t3jmp+OUvcwMMQ='\" -o main ./main.go Your new binary saved as main with new encrypted and embedded default config. To turn on decryption new config you should pass encryption keys as list of keys separated with & symbol: export ENCRYPTION_KEYS = 'some long password to encrypt config&another key' It will override default encryption keys","title":"Embedding encrypted config as default backup config into binary"},{"location":"advanced-docs/config-encryption/#embedding-encryption-keys-into-binary","text":"You can embed keys into binary in same way: export ENCRYPTION_KEYS = 'some long password to encrypt config&another key' make build_encrypted GOOS = linux GOARCH = amd64 CGO_ENABLED = 0 go build -ldflags = \"-X 'github.com/Arriven/db1000n/src/utils.EncryptionKeys=some long password to encrypt config&another key' -X 'github.com/Arriven/db1000n/src/job/config.DefaultConfig=YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IHNjcnlwdCAwS1pOUXlLM004L1NxcDRwL21CaUl3IDE4Cjc1cEZtcmZZZHJieFRvd0hhM0RVVVMxb2VMY0RmQzBkUkJQMXE2UWdyMUEKLS0tIHlSK1VFMkNOSHovbzRqUlJ3RW5VclphRy9TU0NQSG8vMzJUZ1c4RUozZncKD6zE4MONozWBfQYn9HG31DW100o2oFpn6iACQAvCDyXkgSeuQtRFjPwCIW5q2Dltq7Srkc8b81/ZynC59uqkmDJefGyNPzTk3ilRl6wcLOhCP1TD7YtCtZ/7ZpoGpNMiDD6XhKnOmz10sBSy1SXt54+zFVcuQ1ITRi4E2WmiFRjTa8T+ZMwurW+F+iwOu6+z8/0sKQaG5SrKA74GI9D6iRQnqiPg2Abr97Vq7X2Fjvz2NqFjcB0dD29XijHcLCdXQ1DcI3gx94SdMmmfeU5ub2ArsH/4nA8XlS7YE7BirUihgHD4/KIr52dc+Fst6i7SBH433d/Y3Pmhi89FHY8+sGyPFXNG+SeLLHafcR6bLLGyk0iGa2bZaBqUGovYNojni8KSrLRPXTgCyeNAOS7Gpamwi1Xco7m7nEEmAv9vpEvtOUx83pGBOkgu3oSV0t3jmp+OUvcwMMQ='\" -o main ./main.go","title":"Embedding encryption keys into binary"},{"location":"advanced-docs/configuration/","text":"Configuration \u00b6 Command Line reference \u00b6 Usage of db1000n: -b string raw backup config in case the primary one is unavailable -c string path to config files, separated by a comma, each path can be a web endpoint (default \"https://raw.githubusercontent.com/db1000n-coordinators/LoadTestConfig/main/config.v0.7.json\") -country-list string comma-separated list of countries (default \"Ukraine\") -debug enable debug level logging -enable-primitive set to true if you want to run primitive jobs that are less resource-efficient (default true) -enable-self-update Enable the application automatic updates on the startup -format string config format (default \"yaml\") -h print help message and exit -pprof string enable pprof -prometheus_gateways string Comma separated list of prometheus push gateways (default \"https://178.62.78.144:9091,https://46.101.26.43:9091,https://178.62.33.149:9091\") -prometheus_on Start metrics exporting via HTTP and pushing to gateways (specified via <prometheus_gateways>) (default true) -proxy string system proxy to set by default (can be a comma-separated list or a template) -refresh-interval duration refresh timeout for updating the config (default 1m0s) -restart-on-update Allows application to restart upon successful update (ignored if auto-update is disabled) (default true) -scale int used to scale the amount of jobs being launched, effect is similar to launching multiple instances at once (default 1) -self-update-check-frequency duration How often to run auto-update checks (default 24h0m0s) -skip-encrypted set to true if you want to only run plaintext jobs from the config for security considerations -skip-update-check-on-start Allows to skip the update check at the startup (usually set automatically by the previous version) -strict-country-check enable strict country check; will also exit if IP can't be determined -updater-destination-config string Destination config file to write (only applies if updater-mode is enabled (default \"config/config.json\") -updater-mode Only run config updater Almost all of these parameters can also be set via environment variables Config file reference \u00b6 This doc gets outdated frequently as the project is under active development but you can always check up to date configuration examples in examples/config folder The config is expected to be in json format and has following configuration values: jobs - [array] array of attack job definitions to run, should be defined inside the root object jobs[*] - [object] single job definition as json object jobs[*].type - [string] type of the job (determines which attack function to launch). Can be http , tcp , udp , syn-flood , or packetgen jobs[*].count - [number] the amount of instances of the job to be launched, automatically set to 1 if no or invalid value is specified jobs[*].args - [object] arguments to pass to the job. Depends on jobs[*].type http args: request - [object] defines requests to be sent request.method - [string] http method to use (passed directly to go http.NewRequest ) request.path - [string] url path to use (passed directly to go http.NewRequest ) request.body - [object] http payload to use (passed directly to go http.NewRequest ) request.headers - [object] key-value map of http headers request.cookies - [object] key-value map of http cookies (you can still set cookies directly via the header with cookie_string template function or statically, see examples/config/advanced/ddos-guard.yaml for an example) client - [object] http client config for the job client.tls_config - [object] tls config for transport (InsecureSkipVerify is true by default) client.proxy_urls - [array] comma-separated list of string urls for proxies to use (chosen randomly for each request) client.timeout - [time.Duration] client.max_idle_connections - [number] tcp args: proxy_urls - [string] comma-separated list of http/socks5 proxies to use (chosen randomly for each job) timeout - [duration] timeout for connecting to the proxy tcp and udp shared args: address - [string] network host to connect to, can be either hostname:port or ip:port body - [object] json data to be repeatedly sent over the network Warning: packetgen requires root privileges to run packetgen args: connection - [object] raw ip connection parameters connection.name - [string] name of the network to use. can be ip4:tcp , ip6:tcp , ip4:udp , ip6:udp , or anything else supported by the go runtime connection.address - [string] address of the interface used to send packets (on the attacking machine) packet - [object] packet configuration parameters. see examples/config/advanced/packetgen-* for usage examples as there are just too many params to put them here. I'll only describe the general structure of the packet packet.link - [layer] tcp/ip level 1 (OSI level 2) configuration. currently only supports ethernet serialization but go runtime doesn't have a way to send custom ethernet frames so it's not advised to use it packet.network - [layer] tcp/ip level 2 (OSI level 3) configuration. supports ipv4 and ipv6 protocols. see src/core/packetgen/network.go for all the available options packet.transport - [layer] tcp/ip level 3 (OSI level 4) configuration. supports tcp and udp protocols. see src/core/packetgen/transport.go for all the available options packet.payload - [layer] the data that goes on top of other layers. for now it can be raw for custom crafted payload string (i.e. you can write an http request directly here), dns , and icmpv4 , but last two are not fully tested yet all the jobs have shared args: interval_ms - [number] interval between requests in milliseconds. Defaults to 0 (Care, in case of udp job it might generate the data faster than your OS/network card can process it) count - [number] limit the amount of requests to send with this job invocation. Defaults to 0 (no limit). Note: if config is refreshed before this limit is reached the job will be restarted and the counter will be reset Almost every leaf [string] or [object] parameter can be templated with go template syntax. I've also added couple helper functions (list will be growing): random_uuid random_int_n\" random_int random_payload random_ip random_port random_mac_addr random_user_agent local_ip local_ipv4 local_ipv6 local_mac_addr resolve_host resolve_host_ipv4 resolve_host_ipv6 base64_encode base64_decode to_yaml from_yaml from_yaml_array to_json from_json from_json_array from_string_array join split get_url mod ctx_key split cookie_string Please refer to official go documentation and code in src/utils/templates/ for these for now","title":"Configuration"},{"location":"advanced-docs/configuration/#configuration","text":"","title":"Configuration"},{"location":"advanced-docs/configuration/#command-line-reference","text":"Usage of db1000n: -b string raw backup config in case the primary one is unavailable -c string path to config files, separated by a comma, each path can be a web endpoint (default \"https://raw.githubusercontent.com/db1000n-coordinators/LoadTestConfig/main/config.v0.7.json\") -country-list string comma-separated list of countries (default \"Ukraine\") -debug enable debug level logging -enable-primitive set to true if you want to run primitive jobs that are less resource-efficient (default true) -enable-self-update Enable the application automatic updates on the startup -format string config format (default \"yaml\") -h print help message and exit -pprof string enable pprof -prometheus_gateways string Comma separated list of prometheus push gateways (default \"https://178.62.78.144:9091,https://46.101.26.43:9091,https://178.62.33.149:9091\") -prometheus_on Start metrics exporting via HTTP and pushing to gateways (specified via <prometheus_gateways>) (default true) -proxy string system proxy to set by default (can be a comma-separated list or a template) -refresh-interval duration refresh timeout for updating the config (default 1m0s) -restart-on-update Allows application to restart upon successful update (ignored if auto-update is disabled) (default true) -scale int used to scale the amount of jobs being launched, effect is similar to launching multiple instances at once (default 1) -self-update-check-frequency duration How often to run auto-update checks (default 24h0m0s) -skip-encrypted set to true if you want to only run plaintext jobs from the config for security considerations -skip-update-check-on-start Allows to skip the update check at the startup (usually set automatically by the previous version) -strict-country-check enable strict country check; will also exit if IP can't be determined -updater-destination-config string Destination config file to write (only applies if updater-mode is enabled (default \"config/config.json\") -updater-mode Only run config updater Almost all of these parameters can also be set via environment variables","title":"Command Line reference"},{"location":"advanced-docs/configuration/#config-file-reference","text":"This doc gets outdated frequently as the project is under active development but you can always check up to date configuration examples in examples/config folder The config is expected to be in json format and has following configuration values: jobs - [array] array of attack job definitions to run, should be defined inside the root object jobs[*] - [object] single job definition as json object jobs[*].type - [string] type of the job (determines which attack function to launch). Can be http , tcp , udp , syn-flood , or packetgen jobs[*].count - [number] the amount of instances of the job to be launched, automatically set to 1 if no or invalid value is specified jobs[*].args - [object] arguments to pass to the job. Depends on jobs[*].type http args: request - [object] defines requests to be sent request.method - [string] http method to use (passed directly to go http.NewRequest ) request.path - [string] url path to use (passed directly to go http.NewRequest ) request.body - [object] http payload to use (passed directly to go http.NewRequest ) request.headers - [object] key-value map of http headers request.cookies - [object] key-value map of http cookies (you can still set cookies directly via the header with cookie_string template function or statically, see examples/config/advanced/ddos-guard.yaml for an example) client - [object] http client config for the job client.tls_config - [object] tls config for transport (InsecureSkipVerify is true by default) client.proxy_urls - [array] comma-separated list of string urls for proxies to use (chosen randomly for each request) client.timeout - [time.Duration] client.max_idle_connections - [number] tcp args: proxy_urls - [string] comma-separated list of http/socks5 proxies to use (chosen randomly for each job) timeout - [duration] timeout for connecting to the proxy tcp and udp shared args: address - [string] network host to connect to, can be either hostname:port or ip:port body - [object] json data to be repeatedly sent over the network Warning: packetgen requires root privileges to run packetgen args: connection - [object] raw ip connection parameters connection.name - [string] name of the network to use. can be ip4:tcp , ip6:tcp , ip4:udp , ip6:udp , or anything else supported by the go runtime connection.address - [string] address of the interface used to send packets (on the attacking machine) packet - [object] packet configuration parameters. see examples/config/advanced/packetgen-* for usage examples as there are just too many params to put them here. I'll only describe the general structure of the packet packet.link - [layer] tcp/ip level 1 (OSI level 2) configuration. currently only supports ethernet serialization but go runtime doesn't have a way to send custom ethernet frames so it's not advised to use it packet.network - [layer] tcp/ip level 2 (OSI level 3) configuration. supports ipv4 and ipv6 protocols. see src/core/packetgen/network.go for all the available options packet.transport - [layer] tcp/ip level 3 (OSI level 4) configuration. supports tcp and udp protocols. see src/core/packetgen/transport.go for all the available options packet.payload - [layer] the data that goes on top of other layers. for now it can be raw for custom crafted payload string (i.e. you can write an http request directly here), dns , and icmpv4 , but last two are not fully tested yet all the jobs have shared args: interval_ms - [number] interval between requests in milliseconds. Defaults to 0 (Care, in case of udp job it might generate the data faster than your OS/network card can process it) count - [number] limit the amount of requests to send with this job invocation. Defaults to 0 (no limit). Note: if config is refreshed before this limit is reached the job will be restarted and the counter will be reset Almost every leaf [string] or [object] parameter can be templated with go template syntax. I've also added couple helper functions (list will be growing): random_uuid random_int_n\" random_int random_payload random_ip random_port random_mac_addr random_user_agent local_ip local_ipv4 local_ipv6 local_mac_addr resolve_host resolve_host_ipv4 resolve_host_ipv6 base64_encode base64_decode to_yaml from_yaml from_yaml_array to_json from_json from_json_array from_string_array join split get_url mod ctx_key split cookie_string Please refer to official go documentation and code in src/utils/templates/ for these for now","title":"Config file reference"},{"location":"advanced-docs/docker-vpn/","text":"Docker VPN \u00b6 Setting up VPN for Docker users \u00b6 In case of using a dedicated VPS that has banned public IP, a container with OpenVPN client can be deployed inside the same network as db1000n is in. One of the easy ways to set it up is through the docker-compose. There are few docker-compose examples, see examples/docker . Documentation you can find below: Static Docker Compose \u00b6 openvpn/auth.txt : <your username for OpenVPN> <your password for OpenVPN> Also place your *.ovpn file into openvpn/ directory. You can set multiple configuration files and one of them will be used. Old Docker Compose \u00b6 openvpn/provider01.txt : <your username for OpenVPN provider 01> <your password for OpenVPN provider 01> openvpn/provider02.txt : <your username for OpenVPN provider 02> <your password for OpenVPN provider 02> Also place your provider01.endpoint01.conf , provider01.endpoint02.conf and provider02.endpoint01.conf files into openvpn/ directory. Start \u00b6 docker-compose -f examples/docker/your_docker_file.yml up -d Stop \u00b6 docker-compose -f examples/docker/your_docker_file.yml down","title":"Docker VPN"},{"location":"advanced-docs/docker-vpn/#docker-vpn","text":"","title":"Docker VPN"},{"location":"advanced-docs/docker-vpn/#setting-up-vpn-for-docker-users","text":"In case of using a dedicated VPS that has banned public IP, a container with OpenVPN client can be deployed inside the same network as db1000n is in. One of the easy ways to set it up is through the docker-compose. There are few docker-compose examples, see examples/docker . Documentation you can find below:","title":"Setting up VPN for Docker users"},{"location":"advanced-docs/docker-vpn/#static-docker-compose","text":"openvpn/auth.txt : <your username for OpenVPN> <your password for OpenVPN> Also place your *.ovpn file into openvpn/ directory. You can set multiple configuration files and one of them will be used.","title":"Static Docker Compose"},{"location":"advanced-docs/docker-vpn/#old-docker-compose","text":"openvpn/provider01.txt : <your username for OpenVPN provider 01> <your password for OpenVPN provider 01> openvpn/provider02.txt : <your username for OpenVPN provider 02> <your password for OpenVPN provider 02> Also place your provider01.endpoint01.conf , provider01.endpoint02.conf and provider02.endpoint01.conf files into openvpn/ directory.","title":"Old Docker Compose"},{"location":"advanced-docs/docker-vpn/#start","text":"docker-compose -f examples/docker/your_docker_file.yml up -d","title":"Start"},{"location":"advanced-docs/docker-vpn/#stop","text":"docker-compose -f examples/docker/your_docker_file.yml down","title":"Stop"},{"location":"advanced-docs/pull-request-template/","text":"Pull request template \u00b6 Please include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change. Fixes # (issue) Type of change \u00b6 Please delete options that are not relevant. [ ] Bug fix (non-breaking change which fixes an issue) [ ] New feature (non-breaking change which adds functionality) [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected) [ ] Documentation update How Has This Been Tested? \u00b6 Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration [ ] Test A [ ] Test B Test Configuration \u00b6 Release version: Platform: Logs \u00b6 logs Screenshots \u00b6 [ ] No screenshot","title":"Pull request template"},{"location":"advanced-docs/pull-request-template/#pull-request-template","text":"Please include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change. Fixes # (issue)","title":"Pull request template"},{"location":"advanced-docs/pull-request-template/#type-of-change","text":"Please delete options that are not relevant. [ ] Bug fix (non-breaking change which fixes an issue) [ ] New feature (non-breaking change which adds functionality) [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected) [ ] Documentation update","title":"Type of change"},{"location":"advanced-docs/pull-request-template/#how-has-this-been-tested","text":"Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration [ ] Test A [ ] Test B","title":"How Has This Been Tested?"},{"location":"advanced-docs/pull-request-template/#test-configuration","text":"Release version: Platform:","title":"Test Configuration"},{"location":"advanced-docs/pull-request-template/#logs","text":"logs","title":"Logs"},{"location":"advanced-docs/pull-request-template/#screenshots","text":"[ ] No screenshot","title":"Screenshots"},{"location":"advanced-docs/kubernetes/helm-charts/","text":"Helm charts \u00b6 If you want to use plain manifests, see Manifests \u00b6 This is a Helm chart for Kubernetes Prerequisites \u00b6 Make sure that you installed helm package on your local machine and you have connection to the Kubernetes cluster. Install a release \u00b6 cd kubernetes/helm-charts/ helm upgrade --install \\ --create-namespace \\ --namespace = db1000n \\ -f values.yaml db1000n . Destroy a release \u00b6 helm uninstall db1000n -n db1000n","title":"Helm charts"},{"location":"advanced-docs/kubernetes/helm-charts/#helm-charts","text":"","title":"Helm charts"},{"location":"advanced-docs/kubernetes/helm-charts/#if-you-want-to-use-plain-manifests-see-manifests","text":"This is a Helm chart for Kubernetes","title":"If you want to use plain manifests, see Manifests"},{"location":"advanced-docs/kubernetes/helm-charts/#prerequisites","text":"Make sure that you installed helm package on your local machine and you have connection to the Kubernetes cluster.","title":"Prerequisites"},{"location":"advanced-docs/kubernetes/helm-charts/#install-a-release","text":"cd kubernetes/helm-charts/ helm upgrade --install \\ --create-namespace \\ --namespace = db1000n \\ -f values.yaml db1000n .","title":"Install a release"},{"location":"advanced-docs/kubernetes/helm-charts/#destroy-a-release","text":"helm uninstall db1000n -n db1000n","title":"Destroy a release"},{"location":"advanced-docs/kubernetes/manifests/","text":"Manifests install \u00b6 If you use Helm, see our Helm Chart \u00b6 There are two ways to deploy it with plain manifests: using Deployment using DaemonSet Deployment \u00b6 Install: cd kubernetes/manifests/ kubectl apply -f deployment.yaml kubectl get po -n db1000n Scale: kubectl scale deployment/db1000n --replicas = 10 -n db1000n Destroy: kubectl delete deploy db1000n -n db1000n DaemonSet \u00b6 Get and label nodes where you need to run db1000n . There should be nodes at least with 2CPU and 2GB of RAM, CPU resources in priority for db1000n : kubectl get nodes Select nodes where you want to run db1000n from the output and label them: kubectl label nodes <YOUR_UNIQUE_NODE_NAME> db1000n = true Install the DaemonSet: kubectl apply -f daemonset.yaml Destroy: kubectl delete daemonset db1000n -n db1000n How it works? DaemonSet will create one db1000n pod on each node that labeled as db1000n=true . It coule be useful in large cluster types that can be autoscaled horizontally, for example, GKE standard k8s cluster from the free tier purposes.","title":"Manifests install"},{"location":"advanced-docs/kubernetes/manifests/#manifests-install","text":"","title":"Manifests install"},{"location":"advanced-docs/kubernetes/manifests/#if-you-use-helm-see-our-helm-chart","text":"There are two ways to deploy it with plain manifests: using Deployment using DaemonSet","title":"If you use Helm, see our Helm Chart"},{"location":"advanced-docs/kubernetes/manifests/#deployment","text":"Install: cd kubernetes/manifests/ kubectl apply -f deployment.yaml kubectl get po -n db1000n Scale: kubectl scale deployment/db1000n --replicas = 10 -n db1000n Destroy: kubectl delete deploy db1000n -n db1000n","title":"Deployment"},{"location":"advanced-docs/kubernetes/manifests/#daemonset","text":"Get and label nodes where you need to run db1000n . There should be nodes at least with 2CPU and 2GB of RAM, CPU resources in priority for db1000n : kubectl get nodes Select nodes where you want to run db1000n from the output and label them: kubectl label nodes <YOUR_UNIQUE_NODE_NAME> db1000n = true Install the DaemonSet: kubectl apply -f daemonset.yaml Destroy: kubectl delete daemonset db1000n -n db1000n How it works? DaemonSet will create one db1000n pod on each node that labeled as db1000n=true . It coule be useful in large cluster types that can be autoscaled horizontally, for example, GKE standard k8s cluster from the free tier purposes.","title":"DaemonSet"},{"location":"advanced-docs/terraform/aws_ec2/","text":"AWS EC2 \u00b6 Requirements \u00b6 AWS account terraform installed Deploy \u00b6 To deploy run: terraform apply -var-file = \"ireland.tfvars\" You can create new *.tfvars files for different regions and accounts. To swich between regions you can use terraform workspace command. For example: terraform init terraform workspace new eu terraform apply -var-file = \"ireland.tfvars\" terraform workspace new us terraform apply -var-file = \"useast.tfvars\" Destroy \u00b6 To destroy infrastructure use commands: terraform workspace select eu terraform destroy -var-file = \"ireland.tfvars\" terraform workspace select us terraform destroy -var-file = \"useast.tfvars\"","title":"AWS EC2"},{"location":"advanced-docs/terraform/aws_ec2/#aws-ec2","text":"","title":"AWS EC2"},{"location":"advanced-docs/terraform/aws_ec2/#requirements","text":"AWS account terraform installed","title":"Requirements"},{"location":"advanced-docs/terraform/aws_ec2/#deploy","text":"To deploy run: terraform apply -var-file = \"ireland.tfvars\" You can create new *.tfvars files for different regions and accounts. To swich between regions you can use terraform workspace command. For example: terraform init terraform workspace new eu terraform apply -var-file = \"ireland.tfvars\" terraform workspace new us terraform apply -var-file = \"useast.tfvars\"","title":"Deploy"},{"location":"advanced-docs/terraform/aws_ec2/#destroy","text":"To destroy infrastructure use commands: terraform workspace select eu terraform destroy -var-file = \"ireland.tfvars\" terraform workspace select us terraform destroy -var-file = \"useast.tfvars\"","title":"Destroy"},{"location":"advanced-docs/terraform/aws_eks/","text":"AWS EKS \u00b6 Description \u00b6 This implementation allows you to create entire AWS infrastructure from scratch and provides Kubernetes cluster (EKS) to deploy db1000n project. Prerequisites \u00b6 AWS account with AdministratorAccess permissions OS Linux or Windows AWS CLI Terraform Helm kubectl Configure AWS profile \u00b6 The following example shows sample values: $ aws configure AWS Access Key ID [ None ] : AKIAIOSFODNN7EXAMPLE AWS Secret Access Key [ None ] : wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY Default region name [ None ] : us-west-2 Default output format [ None ] : json Deployment \u00b6 Deploy infrastructure \u00b6 cd db1000n/terraform/aws/eks-cluster/ terraform init terraform plan terraform apply NOTE: You can create multilpe *.tfvars configuration files with various variables, regions and AWS accounts using terraform workspace command: cd db1000n/terraform/aws/eks-cluster/ terraform init terraform workspace new $your_workspace terraform plan -var-file $your_file .tfvars terraform apply -var-file $your_file .tfvars Update kubeconfig \u00b6 aws --profile $your_aws_profile eks update-kubeconfig --name $your_eks_cluster_name Connect to EKS cluster \u00b6 $ kubectl get nodes NAME STATUS ROLES AGE VERSION ip-xxx-xxx-x-xx.us-east-1.compute.internal Ready <none> 107m v1.21.5-eks-9017834 ip-xxx-xxx-x-xx.us-east-1.compute.internal Ready <none> 107m v1.21.5-eks-9017834 ip-xxx-xxx-x-xx.us-east-1.compute.internal Ready <none> 107m v1.21.5-eks-9017834 Install application \u00b6 $ cd db1000n/kubernetes/helm-charts/ $ helm upgrade --install \\ --create-namespace \\ --namespace = db1000n \\ -f values.yaml db1000n . Check installation \u00b6 $ kubectl -n db1000n get pods NAME READY STATUS RESTARTS AGE db1000n-54d8744b54-8hffr 1 /1 Running 0 2m10s db1000n-54d8744b54-8vml4 1 /1 Running 0 2m10s db1000n-54d8744b54-9stzv 1 /1 Running 0 2m10s Deletion \u00b6 Delete application \u00b6 helm uninstall db1000n -n db1000n Delete infrastructure \u00b6 terraform destroy","title":"AWS EKS"},{"location":"advanced-docs/terraform/aws_eks/#aws-eks","text":"","title":"AWS EKS"},{"location":"advanced-docs/terraform/aws_eks/#description","text":"This implementation allows you to create entire AWS infrastructure from scratch and provides Kubernetes cluster (EKS) to deploy db1000n project.","title":"Description"},{"location":"advanced-docs/terraform/aws_eks/#prerequisites","text":"AWS account with AdministratorAccess permissions OS Linux or Windows AWS CLI Terraform Helm kubectl","title":"Prerequisites"},{"location":"advanced-docs/terraform/aws_eks/#configure-aws-profile","text":"The following example shows sample values: $ aws configure AWS Access Key ID [ None ] : AKIAIOSFODNN7EXAMPLE AWS Secret Access Key [ None ] : wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY Default region name [ None ] : us-west-2 Default output format [ None ] : json","title":"Configure AWS profile"},{"location":"advanced-docs/terraform/aws_eks/#deployment","text":"","title":"Deployment"},{"location":"advanced-docs/terraform/aws_eks/#deploy-infrastructure","text":"cd db1000n/terraform/aws/eks-cluster/ terraform init terraform plan terraform apply NOTE: You can create multilpe *.tfvars configuration files with various variables, regions and AWS accounts using terraform workspace command: cd db1000n/terraform/aws/eks-cluster/ terraform init terraform workspace new $your_workspace terraform plan -var-file $your_file .tfvars terraform apply -var-file $your_file .tfvars","title":"Deploy infrastructure"},{"location":"advanced-docs/terraform/aws_eks/#update-kubeconfig","text":"aws --profile $your_aws_profile eks update-kubeconfig --name $your_eks_cluster_name","title":"Update kubeconfig"},{"location":"advanced-docs/terraform/aws_eks/#connect-to-eks-cluster","text":"$ kubectl get nodes NAME STATUS ROLES AGE VERSION ip-xxx-xxx-x-xx.us-east-1.compute.internal Ready <none> 107m v1.21.5-eks-9017834 ip-xxx-xxx-x-xx.us-east-1.compute.internal Ready <none> 107m v1.21.5-eks-9017834 ip-xxx-xxx-x-xx.us-east-1.compute.internal Ready <none> 107m v1.21.5-eks-9017834","title":"Connect to EKS cluster"},{"location":"advanced-docs/terraform/aws_eks/#install-application","text":"$ cd db1000n/kubernetes/helm-charts/ $ helm upgrade --install \\ --create-namespace \\ --namespace = db1000n \\ -f values.yaml db1000n .","title":"Install application"},{"location":"advanced-docs/terraform/aws_eks/#check-installation","text":"$ kubectl -n db1000n get pods NAME READY STATUS RESTARTS AGE db1000n-54d8744b54-8hffr 1 /1 Running 0 2m10s db1000n-54d8744b54-8vml4 1 /1 Running 0 2m10s db1000n-54d8744b54-9stzv 1 /1 Running 0 2m10s","title":"Check installation"},{"location":"advanced-docs/terraform/aws_eks/#deletion","text":"","title":"Deletion"},{"location":"advanced-docs/terraform/aws_eks/#delete-application","text":"helm uninstall db1000n -n db1000n","title":"Delete application"},{"location":"advanced-docs/terraform/aws_eks/#delete-infrastructure","text":"terraform destroy","title":"Delete infrastructure"},{"location":"advanced-docs/terraform/aws_lightsail/","text":"AWS Lightsail \u00b6 Requirements \u00b6 AWS account terraform (1.0+) installed Deploy \u00b6 To deploy: terraform init terraform plan terraform apply Destroy \u00b6 To destroy: terraform destroy","title":"AWS Lightsail"},{"location":"advanced-docs/terraform/aws_lightsail/#aws-lightsail","text":"","title":"AWS Lightsail"},{"location":"advanced-docs/terraform/aws_lightsail/#requirements","text":"AWS account terraform (1.0+) installed","title":"Requirements"},{"location":"advanced-docs/terraform/aws_lightsail/#deploy","text":"To deploy: terraform init terraform plan terraform apply","title":"Deploy"},{"location":"advanced-docs/terraform/aws_lightsail/#destroy","text":"To destroy: terraform destroy","title":"Destroy"},{"location":"advanced-docs/terraform/azure/","text":"Azure \u00b6 Prerequisites \u00b6 Install terraform Have Azure account. Prepare environment for Azure Provider The easiest option for auth is Azure CLI Deployment \u00b6 The composition creates container instances in 6 different regions for a broader attack simulation. If you want to make a different setup, just alter modules in the main.tf . Create a new terraform.tfvars file in the folder, if you want to change the default configuration of the farm ( db1000n can be configured with either command line parameters or environment variables, former having precedence over the latter): bomblet_count=10 - can be used for custom number of containers per region attack_commands=[\"/usr/src/app/db1000n\",\"-c=https://link_to_your_config_file\"] attack_environment_variables={\"ENABLE_PRIMITIVE\":\"false\"} terraform init - to restore all dependencies. terraform apply -auto-approve - to provision the attack farm. Collecting logs from the containers \u00b6 The container instances are provisioned without public IP addresses to make the setup more cost effective. If you deploy more than one container per region, play with the -01 suffix to get logs from the correct instance. Logs from North Europe region: az container logs --resource-group main-rg --name main-northeurope-01 --container-name main Logs from West Europe region: az container logs --resource-group main-rg --name main-westeurope-01 --container-name main Logs from Canada Central region: az container logs --resource-group main-rg --name main-canadacentral-01 --container-name main Logs from UAE North region: az container logs --resource-group main-rg --name main-uaenorth-01 --container-name main Logs from Central US region: az container logs --resource-group main-rg --name main-centralus-01 --container-name main Logs from East Asia region: az container logs --resource-group main-rg --name main-eastasia-01 --container-name main Cleanup \u00b6 terraform destroy","title":"Azure"},{"location":"advanced-docs/terraform/azure/#azure","text":"","title":"Azure"},{"location":"advanced-docs/terraform/azure/#prerequisites","text":"Install terraform Have Azure account. Prepare environment for Azure Provider The easiest option for auth is Azure CLI","title":"Prerequisites"},{"location":"advanced-docs/terraform/azure/#deployment","text":"The composition creates container instances in 6 different regions for a broader attack simulation. If you want to make a different setup, just alter modules in the main.tf . Create a new terraform.tfvars file in the folder, if you want to change the default configuration of the farm ( db1000n can be configured with either command line parameters or environment variables, former having precedence over the latter): bomblet_count=10 - can be used for custom number of containers per region attack_commands=[\"/usr/src/app/db1000n\",\"-c=https://link_to_your_config_file\"] attack_environment_variables={\"ENABLE_PRIMITIVE\":\"false\"} terraform init - to restore all dependencies. terraform apply -auto-approve - to provision the attack farm.","title":"Deployment"},{"location":"advanced-docs/terraform/azure/#collecting-logs-from-the-containers","text":"The container instances are provisioned without public IP addresses to make the setup more cost effective. If you deploy more than one container per region, play with the -01 suffix to get logs from the correct instance. Logs from North Europe region: az container logs --resource-group main-rg --name main-northeurope-01 --container-name main Logs from West Europe region: az container logs --resource-group main-rg --name main-westeurope-01 --container-name main Logs from Canada Central region: az container logs --resource-group main-rg --name main-canadacentral-01 --container-name main Logs from UAE North region: az container logs --resource-group main-rg --name main-uaenorth-01 --container-name main Logs from Central US region: az container logs --resource-group main-rg --name main-centralus-01 --container-name main Logs from East Asia region: az container logs --resource-group main-rg --name main-eastasia-01 --container-name main","title":"Collecting logs from the containers"},{"location":"advanced-docs/terraform/azure/#cleanup","text":"terraform destroy","title":"Cleanup"},{"location":"advanced-docs/terraform/digital-ocean/","text":"Digital Ocean \u00b6 Requirements \u00b6 Digital Ocean account API token (Go to API - Personal access tokens) and generate Personal access token (with write permissions) terraform (1.0+) installed Deploy \u00b6 To deploy: export DO_TOKEN = <place-api-token-here> terraform init terraform plan -var \"do_token= ${ DO_TOKEN } \" terraform apply -var \"do_token= ${ DO_TOKEN } \" After deployment (usually takes 5-10 mins) go to Apps List , find an app with name db1000n and chek Runtime Logs. Destroy \u00b6 To destroy: export DO_TOKEN = <place-api-token-here> terraform destroy -var \"do_token= ${ DO_TOKEN } \"","title":"Digital Ocean"},{"location":"advanced-docs/terraform/digital-ocean/#digital-ocean","text":"","title":"Digital Ocean"},{"location":"advanced-docs/terraform/digital-ocean/#requirements","text":"Digital Ocean account API token (Go to API - Personal access tokens) and generate Personal access token (with write permissions) terraform (1.0+) installed","title":"Requirements"},{"location":"advanced-docs/terraform/digital-ocean/#deploy","text":"To deploy: export DO_TOKEN = <place-api-token-here> terraform init terraform plan -var \"do_token= ${ DO_TOKEN } \" terraform apply -var \"do_token= ${ DO_TOKEN } \" After deployment (usually takes 5-10 mins) go to Apps List , find an app with name db1000n and chek Runtime Logs.","title":"Deploy"},{"location":"advanced-docs/terraform/digital-ocean/#destroy","text":"To destroy: export DO_TOKEN = <place-api-token-here> terraform destroy -var \"do_token= ${ DO_TOKEN } \"","title":"Destroy"},{"location":"advanced-docs/terraform/gcp/","text":"GCP \u00b6 Requirements \u00b6 GCP account Subscription on expressvpn.com (get the activation code) terraform installed Init \u00b6 To init Terraform run: terraform init Need to create terraform/gcp_expressvpn/terraform.tfvars file and set two variables values tou yours project_id = \"google-project-id\" expressvpn_key = \"expressvpn-activation-code\" Other vars can be overwritten in this file id needed. Deploy \u00b6 To deploy run: terraform apply Destroy \u00b6 To destroy infrastructure use commands: terraform destroy","title":"GCP"},{"location":"advanced-docs/terraform/gcp/#gcp","text":"","title":"GCP"},{"location":"advanced-docs/terraform/gcp/#requirements","text":"GCP account Subscription on expressvpn.com (get the activation code) terraform installed","title":"Requirements"},{"location":"advanced-docs/terraform/gcp/#init","text":"To init Terraform run: terraform init Need to create terraform/gcp_expressvpn/terraform.tfvars file and set two variables values tou yours project_id = \"google-project-id\" expressvpn_key = \"expressvpn-activation-code\" Other vars can be overwritten in this file id needed.","title":"Init"},{"location":"advanced-docs/terraform/gcp/#deploy","text":"To deploy run: terraform apply","title":"Deploy"},{"location":"advanced-docs/terraform/gcp/#destroy","text":"To destroy infrastructure use commands: terraform destroy","title":"Destroy"},{"location":"advanced-docs/terraform/heroku/","text":"Heroku \u00b6 Requirements \u00b6 Heroku account API token (Go to Account settings - API Key) and reveal API key terraform (1.0+) installed Deploy \u00b6 To deploy: export EMAIL = <place-email-here> export API_KEY = <place-api-key-here> terraform init terraform plan -var \"email= ${ EMAIL } \" -var \"api_key= ${ API_KEY } \" terraform apply -var \"email= ${ EMAIL } \" -var \"api_key= ${ API_KEY } \" Go to apps list and ensure that application successfully deployed. You can check logs for application with Heroku CLI: https://devcenter.heroku.com/articles/logging#view-logs Destroy \u00b6 To destroy: export EMAIL = <place-email-here> export API_KEY = <place-api-key-here> terraform destroy -var \"email= ${ EMAIL } \" -var \"api_key= ${ API_KEY } \"","title":"Heroku"},{"location":"advanced-docs/terraform/heroku/#heroku","text":"","title":"Heroku"},{"location":"advanced-docs/terraform/heroku/#requirements","text":"Heroku account API token (Go to Account settings - API Key) and reveal API key terraform (1.0+) installed","title":"Requirements"},{"location":"advanced-docs/terraform/heroku/#deploy","text":"To deploy: export EMAIL = <place-email-here> export API_KEY = <place-api-key-here> terraform init terraform plan -var \"email= ${ EMAIL } \" -var \"api_key= ${ API_KEY } \" terraform apply -var \"email= ${ EMAIL } \" -var \"api_key= ${ API_KEY } \" Go to apps list and ensure that application successfully deployed. You can check logs for application with Heroku CLI: https://devcenter.heroku.com/articles/logging#view-logs","title":"Deploy"},{"location":"advanced-docs/terraform/heroku/#destroy","text":"To destroy: export EMAIL = <place-email-here> export API_KEY = <place-api-key-here> terraform destroy -var \"email= ${ EMAIL } \" -var \"api_key= ${ API_KEY } \"","title":"Destroy"},{"location":"advanced-docs/terraform/hetzner-cloud/","text":"Hetzner Cloud \u00b6 Requirements \u00b6 Hetzner Cloud account API token (Go to Project - Security - API Tokens) and create a token terraform (1.0+) installed Deploy \u00b6 To deploy: export HCLOUD_TOKEN = <place-api-token-here> export SSH_PUBLIC_KEY = \"<place-ssh-public-key-here>\" terraform init terraform plan -var \"hcloud_token= ${ HCLOUD_TOKEN } \" -var \"ssh_public_key= ${ SSH_PUBLIC_KEY } \" terraform apply -var \"hcloud_token= ${ HCLOUD_TOKEN } \" -var \"ssh_public_key= ${ SSH_PUBLIC_KEY } \" Destroy \u00b6 To destroy: export HCLOUD_TOKEN = <place-api-token-here> export SSH_PUBLIC_KEY = \"<place-ssh-public-key-here>\" terraform destroy -var \"hcloud_token= ${ HCLOUD_TOKEN } \" -var \"ssh_public_key= ${ SSH_PUBLIC_KEY } \"","title":"Hetzner Cloud"},{"location":"advanced-docs/terraform/hetzner-cloud/#hetzner-cloud","text":"","title":"Hetzner Cloud"},{"location":"advanced-docs/terraform/hetzner-cloud/#requirements","text":"Hetzner Cloud account API token (Go to Project - Security - API Tokens) and create a token terraform (1.0+) installed","title":"Requirements"},{"location":"advanced-docs/terraform/hetzner-cloud/#deploy","text":"To deploy: export HCLOUD_TOKEN = <place-api-token-here> export SSH_PUBLIC_KEY = \"<place-ssh-public-key-here>\" terraform init terraform plan -var \"hcloud_token= ${ HCLOUD_TOKEN } \" -var \"ssh_public_key= ${ SSH_PUBLIC_KEY } \" terraform apply -var \"hcloud_token= ${ HCLOUD_TOKEN } \" -var \"ssh_public_key= ${ SSH_PUBLIC_KEY } \"","title":"Deploy"},{"location":"advanced-docs/terraform/hetzner-cloud/#destroy","text":"To destroy: export HCLOUD_TOKEN = <place-api-token-here> export SSH_PUBLIC_KEY = \"<place-ssh-public-key-here>\" terraform destroy -var \"hcloud_token= ${ HCLOUD_TOKEN } \" -var \"ssh_public_key= ${ SSH_PUBLIC_KEY } \"","title":"Destroy"},{"location":"advanced-docs/terraform/vultr/","text":"Vultr \u00b6 Requirements \u00b6 Vultr account API token terraform Deploy \u00b6 export VULTR_API_KEY = \"Your Vultr API Key\" terraform init terraform plan -var \"key=<path_to_ssh_key>\" -var \"num_inst=<number of instances to create>\" terraform apply -var \"key=<path_to_ssh_key>\" -var \"num_inst=<number of instances to create>\" Destroy \u00b6 To delete all the resources that were created run terraform destroy Tips \u00b6 Deploy script installs vnstat util that is useful for monitoring server network performance. Example, get network statistics for the last 5 hours: ssh root@ip vnstat -h 5","title":"Vultr"},{"location":"advanced-docs/terraform/vultr/#vultr","text":"","title":"Vultr"},{"location":"advanced-docs/terraform/vultr/#requirements","text":"Vultr account API token terraform","title":"Requirements"},{"location":"advanced-docs/terraform/vultr/#deploy","text":"export VULTR_API_KEY = \"Your Vultr API Key\" terraform init terraform plan -var \"key=<path_to_ssh_key>\" -var \"num_inst=<number of instances to create>\" terraform apply -var \"key=<path_to_ssh_key>\" -var \"num_inst=<number of instances to create>\"","title":"Deploy"},{"location":"advanced-docs/terraform/vultr/#destroy","text":"To delete all the resources that were created run terraform destroy","title":"Destroy"},{"location":"advanced-docs/terraform/vultr/#tips","text":"Deploy script installs vnstat util that is useful for monitoring server network performance. Example, get network statistics for the last 5 hours: ssh root@ip vnstat -h 5","title":"Tips"},{"location":"uk/","text":"Quick start \u00b6 Death by 1000 needles \u00b6 On 24th of February Russia has launched a full-blown invasion on Ukrainian territory. We're doing our best to stop it and prevent innocent lives being taken Attention Please check existing issues (both open and closed) before creating new ones. It will save me some time answering duplicated questions and right now time is the most critical resource. Regards. Quickstart guide \u00b6 Attention This tool is responsible only for traffic generation, you may want to use VPN if you want to test geo-blocking For dummies \u00b6 Download an application for your platform: Windows Mac M1 Mac Intel Linux 32bit Linux 64bit Unpack the archive Launch the file inside the archive Done! Important Cloud providers could charge a huge amount of money not only for compute resources but for traffic as well. If you run an app in the cloud please control your billing (only advanced users are affected)! Info You can get warnings from your computer about the file - ignore them (or allow in System Settings). Our software is open source. It can be checked and compiled by you yourself. How to install db1000n \u00b6 There are different ways to install and run db1000n Binary file \u00b6 Download the latest release for your arch/OS. Unpack the archive and run it Docker \u00b6 If you already have installed Docker, just run: docker run --rm -it --pull always ghcr.io/arriven/db1000n Or, if your container is not able to connect to your local VPN: docker run --rm -it --pull always --network host ghcr.io/arriven/db1000n Advanced users \u00b6 See For advanced I still have questions \u00b6 You will find some answers on our FAQ","title":"Quick start"},{"location":"uk/#quick-start","text":"","title":"Quick start"},{"location":"uk/#death-by-1000-needles","text":"On 24th of February Russia has launched a full-blown invasion on Ukrainian territory. We're doing our best to stop it and prevent innocent lives being taken Attention Please check existing issues (both open and closed) before creating new ones. It will save me some time answering duplicated questions and right now time is the most critical resource. Regards.","title":"Death by 1000 needles"},{"location":"uk/#quickstart-guide","text":"Attention This tool is responsible only for traffic generation, you may want to use VPN if you want to test geo-blocking","title":"Quickstart guide"},{"location":"uk/#for-dummies","text":"Download an application for your platform: Windows Mac M1 Mac Intel Linux 32bit Linux 64bit Unpack the archive Launch the file inside the archive Done! Important Cloud providers could charge a huge amount of money not only for compute resources but for traffic as well. If you run an app in the cloud please control your billing (only advanced users are affected)! Info You can get warnings from your computer about the file - ignore them (or allow in System Settings). Our software is open source. It can be checked and compiled by you yourself.","title":"For dummies"},{"location":"uk/#how-to-install-db1000n","text":"There are different ways to install and run db1000n","title":"How to install db1000n"},{"location":"uk/#binary-file","text":"Download the latest release for your arch/OS. Unpack the archive and run it","title":"Binary file"},{"location":"uk/#docker","text":"If you already have installed Docker, just run: docker run --rm -it --pull always ghcr.io/arriven/db1000n Or, if your container is not able to connect to your local VPN: docker run --rm -it --pull always --network host ghcr.io/arriven/db1000n","title":"Docker"},{"location":"uk/#advanced-users","text":"See For advanced","title":"Advanced users"},{"location":"uk/#i-still-have-questions","text":"You will find some answers on our FAQ","title":"I still have questions"},{"location":"uk/faq/","text":"FAQ \u00b6 Where can I find advanced documentation? Here I installed db1000n but it's not working properly. What to do? Create Issue and community will help you with solving a problem I'm not a developer, how can I help to project? Share information about db1000n in social media, with your friends and colleagues Run db1000n on every possible platform (local machine, public clouds, Docker, Kubernetes, etc) Create Issues or Pull Requests if you found any bugs, missed documentation, misspells, etc I'm a developer, how can I help to project? Check Issues to help with important tasks Check our codebase and make PRs Test an app on different platforms and report bugs or feature requests When I run db1000n I see that it generates low amount of traffic. Isn't that bad? it's okay The app is configurable to generate set amount of traffic (controlled by the number of targets, their type, and attack interval for each of them). The main reason it works that way is because there are two main types of ddos: Straightforward load generation (easy to implement, easy to defend from) - as effective as the amount of raw traffic you can generate Actual denial of service that aims to remain as undetected as possible by simulating plausible traffic and hitting concrete vulnerabilities in the target (or targets). This type of ddos doesn't require a lot of traffic and thus is mostly limited by the amount of clients generating this type of load (or rather unique IPs) Should I care about costs if I run an app in public cloud? Yes Cloud providers could charge a huge amount of money not only for compute resources but for traffic as well. If you run an app in the cloud please control your billing Can I leave the site for the night? Yes, you can. I personally leave the browser on overnight and it works fine. How can I make sure that the computer does not go to sleep while the site is running? To do this, you need to install a program which keeps the screen turned off. Instructions for different operating systems below: I have Windows: Caffeinated ( download ) I have Mac OS: Amphetamine ( download ) What are primitive jobs? Primitive jobs rely on generating as much raw traffic as possible. This might exhaust your system. They are also easier to detect and unadvisable to be used in the cloud environment. The app shows low response rate, is it ok? Low response rate alone is not enough to be a problem as it could be an indication that current targets are down but you can try to perform additional checks in case you think the rate is abnormal (trying to access one of the targets via curl/browser, checking network stats via other tools like bmon/Task manager, enabling and inspecting debug logs, etc.)","title":"FAQ"},{"location":"uk/faq/#faq","text":"Where can I find advanced documentation? Here I installed db1000n but it's not working properly. What to do? Create Issue and community will help you with solving a problem I'm not a developer, how can I help to project? Share information about db1000n in social media, with your friends and colleagues Run db1000n on every possible platform (local machine, public clouds, Docker, Kubernetes, etc) Create Issues or Pull Requests if you found any bugs, missed documentation, misspells, etc I'm a developer, how can I help to project? Check Issues to help with important tasks Check our codebase and make PRs Test an app on different platforms and report bugs or feature requests When I run db1000n I see that it generates low amount of traffic. Isn't that bad? it's okay The app is configurable to generate set amount of traffic (controlled by the number of targets, their type, and attack interval for each of them). The main reason it works that way is because there are two main types of ddos: Straightforward load generation (easy to implement, easy to defend from) - as effective as the amount of raw traffic you can generate Actual denial of service that aims to remain as undetected as possible by simulating plausible traffic and hitting concrete vulnerabilities in the target (or targets). This type of ddos doesn't require a lot of traffic and thus is mostly limited by the amount of clients generating this type of load (or rather unique IPs) Should I care about costs if I run an app in public cloud? Yes Cloud providers could charge a huge amount of money not only for compute resources but for traffic as well. If you run an app in the cloud please control your billing Can I leave the site for the night? Yes, you can. I personally leave the browser on overnight and it works fine. How can I make sure that the computer does not go to sleep while the site is running? To do this, you need to install a program which keeps the screen turned off. Instructions for different operating systems below: I have Windows: Caffeinated ( download ) I have Mac OS: Amphetamine ( download ) What are primitive jobs? Primitive jobs rely on generating as much raw traffic as possible. This might exhaust your system. They are also easier to detect and unadvisable to be used in the cloud environment. The app shows low response rate, is it ok? Low response rate alone is not enough to be a problem as it could be an indication that current targets are down but you can try to perform additional checks in case you think the rate is abnormal (trying to access one of the targets via curl/browser, checking network stats via other tools like bmon/Task manager, enabling and inspecting debug logs, etc.)","title":"FAQ"},{"location":"uk/license/","text":"License \u00b6 Copyright (c) 2022 Bohdan Ivashko https://github.com/Arriven Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"\u041b\u0456\u0446\u0435\u043d\u0437\u0456\u044f"},{"location":"uk/license/#license","text":"Copyright (c) 2022 Bohdan Ivashko https://github.com/Arriven Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"uk/over-the-air/","text":"Over the air updates \u00b6 Lots of maintainers run their needles on a bare metal machines. As long as this project is so frequently updated, it might be a good idea to let them update it without the hassle. [x] Enabled automatic time-based version check [x] Enabled application self-restart after it downloaded the update Description \u00b6 Support for the application self-update by downloading the latest release from the official repository. Stay strong, be the first in line! The version should be embedded in the binary during the build, see the build target in the Makefile. Usage \u00b6 Available flags \u00b6 -enable-self-update Enable the application automatic updates on the startup -restart-on-update Allows application to restart upon the successful update ( ignored if auto-update is disabled ) ( default true ) -self-update-check-frequency duration How often to run auto-update checks ( default 24h0m0s ) -skip-update-check-on-start Allows to skip the update check at the startup ( usually set automatically by the previous version ) ( default false ) The default behavior if the self-update enabled: * Check for the update * If update is available - download it * If auto-restart is enabled * Notify the user that a newer version is available * Fork-Exec a new process ( will have a different PID ) , add a flag to skip the version check upon startup * Stop the current process * If auto-restart is disabled - notify user that manual restart is required * If update is NOT available - schedule the next check Examples \u00b6 To update your needle, start it with a flag -enable-self-update ./db1000n -enable-self-update Advanced options \u00b6 Start the needle with the self-update & self-restart $ ./db1000n -enable-self-update 0000 /00/00 00 :00:00 main.go:82: DB1000n [ Version: v0.6.4 ][ PID = 75259 ] 0000 /00/00 00 :00:00 main.go:166: Running a check for a newer version... 0000 /00/00 00 :00:00 main.go:176: Newer version of the application is found [ 0 .7.0 ] 0000 /00/00 00 :00:00 main.go:177: What ' s new: * Added some great improvements * Added some spectacular bugs 0000 /00/00 00 :00:00 main.go:180: Auto restart is enabled, restarting the application to run a new version 0000 /00/00 00 :00:00 restart.go:45: new process has been started successfully [ old_pid = 75259 ,new_pid = 75262 ] # NOTE: Process 75259 exited, Process 75262 has started with a flag to skip version check on the startup 0000 /00/00 00 :00:00 main.go:82: DB1000n [ Version: v0.7.0 ][ PID = 75262 ] 0000 /00/00 00 :00:00 main.go:155: Version update on startup is skipped, next update check is scheduled in 24h0m0s Start the needle with the self-update but do not restart the process upon update ( systemd friendly) $ ./db1000n -enable-self-update -self-update-check-frequency = 5m -restart-on-update = false 0000 /00/00 00 :00:00 main.go:82: DB1000n [ Version: v0.6.4 ][ PID = 75320 ] 0000 /00/00 00 :00:00 main.go:166: Running a check for a newer version... 0000 /00/00 00 :00:00 main.go:176: Newer version of the application is found [ 0 .7.0 ] 0000 /00/00 00 :00:00 main.go:177: What ' s new: * Added some great improvements * Added some spectacular bugs 0000 /00/00 00 :00:00 main.go:191: Auto restart is disabled, restart the application manually to apply changes! References \u00b6 Graceful restart with zero downtime for TCP connection - https://github.com/Scalingo/go-graceful-restart-example Graceful restart with zero downtime for TCP connection (two variants) https://github.com/rcrowley/goagain Graceful restart with zero downtime for TCP connection (alternative) https://grisha.org/blog/2014/06/03/graceful-restart-in-golang","title":"Over the air updates"},{"location":"uk/over-the-air/#over-the-air-updates","text":"Lots of maintainers run their needles on a bare metal machines. As long as this project is so frequently updated, it might be a good idea to let them update it without the hassle. [x] Enabled automatic time-based version check [x] Enabled application self-restart after it downloaded the update","title":"Over the air updates"},{"location":"uk/over-the-air/#description","text":"Support for the application self-update by downloading the latest release from the official repository. Stay strong, be the first in line! The version should be embedded in the binary during the build, see the build target in the Makefile.","title":"Description"},{"location":"uk/over-the-air/#usage","text":"","title":"Usage"},{"location":"uk/over-the-air/#available-flags","text":"-enable-self-update Enable the application automatic updates on the startup -restart-on-update Allows application to restart upon the successful update ( ignored if auto-update is disabled ) ( default true ) -self-update-check-frequency duration How often to run auto-update checks ( default 24h0m0s ) -skip-update-check-on-start Allows to skip the update check at the startup ( usually set automatically by the previous version ) ( default false ) The default behavior if the self-update enabled: * Check for the update * If update is available - download it * If auto-restart is enabled * Notify the user that a newer version is available * Fork-Exec a new process ( will have a different PID ) , add a flag to skip the version check upon startup * Stop the current process * If auto-restart is disabled - notify user that manual restart is required * If update is NOT available - schedule the next check","title":"Available flags"},{"location":"uk/over-the-air/#examples","text":"To update your needle, start it with a flag -enable-self-update ./db1000n -enable-self-update","title":"Examples"},{"location":"uk/over-the-air/#advanced-options","text":"Start the needle with the self-update & self-restart $ ./db1000n -enable-self-update 0000 /00/00 00 :00:00 main.go:82: DB1000n [ Version: v0.6.4 ][ PID = 75259 ] 0000 /00/00 00 :00:00 main.go:166: Running a check for a newer version... 0000 /00/00 00 :00:00 main.go:176: Newer version of the application is found [ 0 .7.0 ] 0000 /00/00 00 :00:00 main.go:177: What ' s new: * Added some great improvements * Added some spectacular bugs 0000 /00/00 00 :00:00 main.go:180: Auto restart is enabled, restarting the application to run a new version 0000 /00/00 00 :00:00 restart.go:45: new process has been started successfully [ old_pid = 75259 ,new_pid = 75262 ] # NOTE: Process 75259 exited, Process 75262 has started with a flag to skip version check on the startup 0000 /00/00 00 :00:00 main.go:82: DB1000n [ Version: v0.7.0 ][ PID = 75262 ] 0000 /00/00 00 :00:00 main.go:155: Version update on startup is skipped, next update check is scheduled in 24h0m0s Start the needle with the self-update but do not restart the process upon update ( systemd friendly) $ ./db1000n -enable-self-update -self-update-check-frequency = 5m -restart-on-update = false 0000 /00/00 00 :00:00 main.go:82: DB1000n [ Version: v0.6.4 ][ PID = 75320 ] 0000 /00/00 00 :00:00 main.go:166: Running a check for a newer version... 0000 /00/00 00 :00:00 main.go:176: Newer version of the application is found [ 0 .7.0 ] 0000 /00/00 00 :00:00 main.go:177: What ' s new: * Added some great improvements * Added some spectacular bugs 0000 /00/00 00 :00:00 main.go:191: Auto restart is disabled, restart the application manually to apply changes!","title":"Advanced options"},{"location":"uk/over-the-air/#references","text":"Graceful restart with zero downtime for TCP connection - https://github.com/Scalingo/go-graceful-restart-example Graceful restart with zero downtime for TCP connection (two variants) https://github.com/rcrowley/goagain Graceful restart with zero downtime for TCP connection (alternative) https://grisha.org/blog/2014/06/03/graceful-restart-in-golang","title":"References"},{"location":"uk/advanced-docs/advanced-and-devs/","text":"Advanced and devs \u00b6 For developers \u00b6 Developed by Arriven . This is a simple distributed load generation client written in go. It is able to fetch simple json config from a local or remote location. The config describes which load generation jobs should be launched in parallel. There are other tools doing that. I do not intend to copy or replace them but rather provide a simple open source alternative so that users have more options. Feel free to use it in your load tests (wink-wink). The software is provided as is under no guarantee. I will update both the repo and this doc as I go during following days (date of writing this is 26th of February 2022, third day of Russian invasion into Ukraine). Go installation \u00b6 Run command in your terminal: go install github.com/Arriven/db1000n@latest ~/go/bin/db1000n Shell installation \u00b6 Run install script directly into the shell (useful for installation through SSH): source < ( curl https://raw.githubusercontent.com/Arriven/db1000n/main/install.sh ) The command above will detect the OS and architecture, download the archive, validate it, and extract db1000n executable into the working directory. You can run it via this command: ./db1000n Docker + OpenVPN \u00b6 How to install docker: https://docs.docker.com/get-docker/ Make sure you've set all available resources to docker: Windows Mac Linux See docker-vpn for instructions on setting it up Kubernetes \u00b6 Here possible ways to deploy into it Helm Chart Manifest Public Clouds \u00b6 See possible ways to deploy into public clouds AWS Azure Digital Ocean Googls Cloud Platform Heroku See also \u00b6 db1000nX100 - a project that automates VPN rotation for db1000n instances that allows you to generate geographically distributed traffic (i.e. to stress test geo-blocking)","title":"Advanced and devs"},{"location":"uk/advanced-docs/advanced-and-devs/#advanced-and-devs","text":"","title":"Advanced and devs"},{"location":"uk/advanced-docs/advanced-and-devs/#for-developers","text":"Developed by Arriven . This is a simple distributed load generation client written in go. It is able to fetch simple json config from a local or remote location. The config describes which load generation jobs should be launched in parallel. There are other tools doing that. I do not intend to copy or replace them but rather provide a simple open source alternative so that users have more options. Feel free to use it in your load tests (wink-wink). The software is provided as is under no guarantee. I will update both the repo and this doc as I go during following days (date of writing this is 26th of February 2022, third day of Russian invasion into Ukraine).","title":"For developers"},{"location":"uk/advanced-docs/advanced-and-devs/#go-installation","text":"Run command in your terminal: go install github.com/Arriven/db1000n@latest ~/go/bin/db1000n","title":"Go installation"},{"location":"uk/advanced-docs/advanced-and-devs/#shell-installation","text":"Run install script directly into the shell (useful for installation through SSH): source < ( curl https://raw.githubusercontent.com/Arriven/db1000n/main/install.sh ) The command above will detect the OS and architecture, download the archive, validate it, and extract db1000n executable into the working directory. You can run it via this command: ./db1000n","title":"Shell installation"},{"location":"uk/advanced-docs/advanced-and-devs/#docker--openvpn","text":"How to install docker: https://docs.docker.com/get-docker/ Make sure you've set all available resources to docker: Windows Mac Linux See docker-vpn for instructions on setting it up","title":"Docker + OpenVPN"},{"location":"uk/advanced-docs/advanced-and-devs/#kubernetes","text":"Here possible ways to deploy into it Helm Chart Manifest","title":"Kubernetes"},{"location":"uk/advanced-docs/advanced-and-devs/#public-clouds","text":"See possible ways to deploy into public clouds AWS Azure Digital Ocean Googls Cloud Platform Heroku","title":"Public Clouds"},{"location":"uk/advanced-docs/advanced-and-devs/#see-also","text":"db1000nX100 - a project that automates VPN rotation for db1000n instances that allows you to generate geographically distributed traffic (i.e. to stress test geo-blocking)","title":"See also"},{"location":"uk/advanced-docs/config-encryption/","text":"Encryption \u00b6 How application uses configuration files according to encryption \u00b6 Encryption is done using age as CLI and golang library for encryption/decryption. Under the hood it uses ChaCha20+Poly1305 as AEAD encryption and store encrypted files with pre-defined header age-encryption.org/v1 When application loads config from any source, at first it check is it encrypted (has header). If it is encrypted then tries to decrypt using keys stored in ENCRYPTION_KEYS env variable and iteratively try every key until successful decryption or skip config. If it wasn't encrypted then return as is. You can encrypt every config file with separate keys and run application specifying all keys at once as a list with separator: & . Separator & was chosen to allow pass base64 strings as keys that can encode random keys (binary values). Encrypt new default config \u00b6 Prepare JSON config \u00b6 Let's take next config for example: { \"jobs\" : [ { \"type\" : \"slow-loris\" , \"args\" : { \"address\" : \"127.0.0.1:53\" , \"ContentLength\" : 1000 , \"DialWorkersCount\" : 1 , \"RampUpInterval\" : 1 , \"SleepInterval\" : 1000 , \"DurationSeconds\" : 1000 , \"Path\" : \"https://example.com\" } } ] } and save it as /home/user/config.json Prepare key for encryption \u00b6 Use some of stored in src/utils/crypto.go.ENCRYPTION_KEYS or generate new one. For example lets use: some long password to encrypt config Encrypt config \u00b6 Using make \u00b6 make DEFAULT_CONFIG = /home/user/config.json encrypt_config Enter passphrase ( leave empty to autogenerate a secure one ) : some long password to encrypt config Confirm passphrase: some long password to encrypt config Saved in file: /tmp/fileMx6JFo Save value as env variable: export DEFAULT_CONFIG_VALUE = 'YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IHNjcnlwdCAwS1pOUXlLM004L1NxcDRwL21CaUl3IDE4Cjc1cEZtcmZZZHJieFRvd0hhM0RVVVMxb2VMY0RmQzBkUkJQMXE2UWdyMUEKLS0tIHlSK1VFMkNOSHovbzRqUlJ3RW5VclphRy9TU0NQSG8vMzJUZ1c4RUozZncKD6zE4MONozWBfQYn9HG31DW100o2oFpn6iACQAvCDyXkgSeuQtRFjPwCIW5q2Dltq7Srkc8b81/ZynC59uqkmDJefGyNPzTk3ilRl6wcLOhCP1TD7YtCtZ/7ZpoGpNMiDD6XhKnOmz10sBSy1SXt54+zFVcuQ1ITRi4E2WmiFRjTa8T+ZMwurW+F+iwOu6+z8/0sKQaG5SrKA74GI9D6iRQnqiPg2Abr97Vq7X2Fjvz2NqFjcB0dD29XijHcLCdXQ1DcI3gx94SdMmmfeU5ub2ArsH/4nA8XlS7YE7BirUihgHD4/KIr52dc+Fst6i7SBH433d/Y3Pmhi89FHY8+sGyPFXNG+SeLLHafcR6bLLGyk0iGa2bZaBqUGovYNojni8KSrLRPXTgCyeNAOS7Gpamwi1Xco7m7nEEmAv9vpEvtOUx83pGBOkgu3oSV0t3jmp+OUvcwMMQ=' Encrypted config file saved to /tmp/fileMx6JFo file. And can be saved and distributed anywhere. Using age \u00b6 age --encrypt -p --output = encrypted_config.json /home/user/config.json Enter passphrase ( leave empty to autogenerate a secure one ) : some long password to encrypt config Confirm passphrase: some long password to encrypt config Converting to base64 \u00b6 cat encrypted_config.json | base64 | tr -d '\\n' YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IHNjcnlwdCAwS1pOUXlLM004L1NxcDRwL21CaUl3IDE4Cjc1cEZtcmZZZHJieFRvd0hhM0RVVVMxb2VMY0RmQzBkUkJQMXE2UWdyMUEKLS0tIHlSK1VFMkNOSHovbzRqUlJ3RW5VclphRy9TU0NQSG8vMzJUZ1c4RUozZncKD6zE4MONozWBfQYn9HG31DW100o2oFpn6iACQAvCDyXkgSeuQtRFjPwCIW5q2Dltq7Srkc8b81/ZynC59uqkmDJefGyNPzTk3ilRl6wcLOhCP1TD7YtCtZ/7ZpoGpNMiDD6XhKnOmz10sBSy1SXt54+zFVcuQ1ITRi4E2WmiFRjTa8T+ZMwurW+F+iwOu6+z8/0sKQaG5SrKA74GI9D6iRQnqiPg2Abr97Vq7X2Fjvz2NqFjcB0dD29XijHcLCdXQ1DcI3gx94SdMmmfeU5ub2ArsH/4nA8XlS7YE7BirUihgHD4/KIr52dc+Fst6i7SBH433d/Y3Pmhi89FHY8+sGyPFXNG+SeLLHafcR6bLLGyk0iGa2bZaBqUGovYNojni8KSrLRPXTgCyeNAOS7Gpamwi1Xco7m7nEEmAv9vpEvtOUx83pGBOkgu3oSV0t3jmp+OUvcwMMQ = Encrypt only part of the config \u00b6 You can encrypt a single job and add it to plaintext config (nothing stopping you from encrypting it further afterwards) via encrypted job entry: { \"jobs\" : [ { \"type\" : \"encrypted\" , \"args\" : { \"format\" : \"json\" , \"data\" : \"YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IHNjcnlwdCB5eCtiMzQ5RWlZRXo4dTNpRE8veHdRIDE4CmYyb2d0YXlnaXptS25sbUJlQUVaUHpRbngwaUdBYUpJRStHbFltdUVNNkUKLS0tIG5oUUVCd041TWJoNWNCQjhvODk4eUFpUldmUFUvaStpanRsdCtWR0RrSVkK2ehc+JYVl+f5VgLKV0mG/J4CQrtHn+FFV5AAcKiLEAjU6MNDaVqBI6Qm9RunLZ51wAA13DLZkPJH39DcsS77H3HmgLpRQ7DMFG2AIDxWysIt2Yi2hVVn9Ogea73twGa8FOpk2kk0Z7NSHCCcpTJd1Db4cwYJiIFaqfBXR+VZtNk3qBgUMStN1CiOyJxvHbnc6tbfeqq042LImKsaLvFzB2y5H/ec9BonHimrP/aZv6dhequs\" } }, { \"type\" : \"tcp\" , \"count\" : 100 , \"args\" : { \"address\" : \"localhost:9090\" , \"body\" : \"more_test\" , \"interval_ms\" : 1000 } } ] } Where args.format represents the encoding format you used for the data before encryption and args.data is a single ecrypted job. To encrypt a single job use same steps as to encrypt the whole config but use a file that contains just the job definition: { \"type\" : \"tcp\" , \"count\" : 100 , \"args\" : { \"address\" : \"localhost:9090\" , \"body\" : \"more_test\" , \"interval_ms\" : 1000 } } Note: each single decryption needs at least 256mb of RAM to set up an encryption key (this is implemented by age as hardening against bruteforce attempts). Due to that encrypting multiple jobs separately may be inefficient and it's advised to use parallel job to group multiple independed jobs into one: { \"type\" : \"parallel\" , \"args\" : { \"jobs\" : [ { \"type\" : \"tcp\" , \"count\" : 100 , \"args\" : { \"address\" : \"localhost:9090\" , \"body\" : \"payload\" , \"interval_ms\" : 1000 } }, { \"type\" : \"tcp\" , \"count\" : 100 , \"args\" : { \"address\" : \"localhost:9091\" , \"body\" : \"payload\" , \"interval_ms\" : 1000 } } ] } } Embedding encrypted config as default backup config into binary \u00b6 Export env variable as it printed: export DEFAULT_CONFIG_VALUE = 'YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IHNjcnlwdCAwS1pOUXlLM004L1NxcDRwL21CaUl3IDE4Cjc1cEZtcmZZZHJieFRvd0hhM0RVVVMxb2VMY0RmQzBkUkJQMXE2UWdyMUEKLS0tIHlSK1VFMkNOSHovbzRqUlJ3RW5VclphRy9TU0NQSG8vMzJUZ1c4RUozZncKD6zE4MONozWBfQYn9HG31DW100o2oFpn6iACQAvCDyXkgSeuQtRFjPwCIW5q2Dltq7Srkc8b81/ZynC59uqkmDJefGyNPzTk3ilRl6wcLOhCP1TD7YtCtZ/7ZpoGpNMiDD6XhKnOmz10sBSy1SXt54+zFVcuQ1ITRi4E2WmiFRjTa8T+ZMwurW+F+iwOu6+z8/0sKQaG5SrKA74GI9D6iRQnqiPg2Abr97Vq7X2Fjvz2NqFjcB0dD29XijHcLCdXQ1DcI3gx94SdMmmfeU5ub2ArsH/4nA8XlS7YE7BirUihgHD4/KIr52dc+Fst6i7SBH433d/Y3Pmhi89FHY8+sGyPFXNG+SeLLHafcR6bLLGyk0iGa2bZaBqUGovYNojni8KSrLRPXTgCyeNAOS7Gpamwi1Xco7m7nEEmAv9vpEvtOUx83pGBOkgu3oSV0t3jmp+OUvcwMMQ=' This base64 value is encrypted config with specified password (hashed with scrypt against brute force). Build new binary with new config: make build_encrypted GOOS = linux GOARCH = amd64 CGO_ENABLED = 0 go build -ldflags = \"-X 'github.com/Arriven/db1000n/src/job/config.DefaultConfig=YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IHNjcnlwdCAwS1pOUXlLM004L1NxcDRwL21CaUl3IDE4Cjc1cEZtcmZZZHJieFRvd0hhM0RVVVMxb2VMY0RmQzBkUkJQMXE2UWdyMUEKLS0tIHlSK1VFMkNOSHovbzRqUlJ3RW5VclphRy9TU0NQSG8vMzJUZ1c4RUozZncKD6zE4MONozWBfQYn9HG31DW100o2oFpn6iACQAvCDyXkgSeuQtRFjPwCIW5q2Dltq7Srkc8b81/ZynC59uqkmDJefGyNPzTk3ilRl6wcLOhCP1TD7YtCtZ/7ZpoGpNMiDD6XhKnOmz10sBSy1SXt54+zFVcuQ1ITRi4E2WmiFRjTa8T+ZMwurW+F+iwOu6+z8/0sKQaG5SrKA74GI9D6iRQnqiPg2Abr97Vq7X2Fjvz2NqFjcB0dD29XijHcLCdXQ1DcI3gx94SdMmmfeU5ub2ArsH/4nA8XlS7YE7BirUihgHD4/KIr52dc+Fst6i7SBH433d/Y3Pmhi89FHY8+sGyPFXNG+SeLLHafcR6bLLGyk0iGa2bZaBqUGovYNojni8KSrLRPXTgCyeNAOS7Gpamwi1Xco7m7nEEmAv9vpEvtOUx83pGBOkgu3oSV0t3jmp+OUvcwMMQ='\" -o main ./main.go Your new binary saved as main with new encrypted and embedded default config. To turn on decryption new config you should pass encryption keys as list of keys separated with & symbol: export ENCRYPTION_KEYS = 'some long password to encrypt config&another key' It will override default encryption keys Embedding encryption keys into binary \u00b6 You can embed keys into binary in same way: export ENCRYPTION_KEYS = 'some long password to encrypt config&another key' make build_encrypted GOOS = linux GOARCH = amd64 CGO_ENABLED = 0 go build -ldflags = \"-X 'github.com/Arriven/db1000n/src/utils.EncryptionKeys=some long password to encrypt config&another key' -X 'github.com/Arriven/db1000n/src/job/config.DefaultConfig=YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IHNjcnlwdCAwS1pOUXlLM004L1NxcDRwL21CaUl3IDE4Cjc1cEZtcmZZZHJieFRvd0hhM0RVVVMxb2VMY0RmQzBkUkJQMXE2UWdyMUEKLS0tIHlSK1VFMkNOSHovbzRqUlJ3RW5VclphRy9TU0NQSG8vMzJUZ1c4RUozZncKD6zE4MONozWBfQYn9HG31DW100o2oFpn6iACQAvCDyXkgSeuQtRFjPwCIW5q2Dltq7Srkc8b81/ZynC59uqkmDJefGyNPzTk3ilRl6wcLOhCP1TD7YtCtZ/7ZpoGpNMiDD6XhKnOmz10sBSy1SXt54+zFVcuQ1ITRi4E2WmiFRjTa8T+ZMwurW+F+iwOu6+z8/0sKQaG5SrKA74GI9D6iRQnqiPg2Abr97Vq7X2Fjvz2NqFjcB0dD29XijHcLCdXQ1DcI3gx94SdMmmfeU5ub2ArsH/4nA8XlS7YE7BirUihgHD4/KIr52dc+Fst6i7SBH433d/Y3Pmhi89FHY8+sGyPFXNG+SeLLHafcR6bLLGyk0iGa2bZaBqUGovYNojni8KSrLRPXTgCyeNAOS7Gpamwi1Xco7m7nEEmAv9vpEvtOUx83pGBOkgu3oSV0t3jmp+OUvcwMMQ='\" -o main ./main.go","title":"Encryption"},{"location":"uk/advanced-docs/config-encryption/#encryption","text":"","title":"Encryption"},{"location":"uk/advanced-docs/config-encryption/#how-application-uses-configuration-files-according-to-encryption","text":"Encryption is done using age as CLI and golang library for encryption/decryption. Under the hood it uses ChaCha20+Poly1305 as AEAD encryption and store encrypted files with pre-defined header age-encryption.org/v1 When application loads config from any source, at first it check is it encrypted (has header). If it is encrypted then tries to decrypt using keys stored in ENCRYPTION_KEYS env variable and iteratively try every key until successful decryption or skip config. If it wasn't encrypted then return as is. You can encrypt every config file with separate keys and run application specifying all keys at once as a list with separator: & . Separator & was chosen to allow pass base64 strings as keys that can encode random keys (binary values).","title":"How application uses configuration files according to encryption"},{"location":"uk/advanced-docs/config-encryption/#encrypt-new-default-config","text":"","title":"Encrypt new default config"},{"location":"uk/advanced-docs/config-encryption/#prepare-json-config","text":"Let's take next config for example: { \"jobs\" : [ { \"type\" : \"slow-loris\" , \"args\" : { \"address\" : \"127.0.0.1:53\" , \"ContentLength\" : 1000 , \"DialWorkersCount\" : 1 , \"RampUpInterval\" : 1 , \"SleepInterval\" : 1000 , \"DurationSeconds\" : 1000 , \"Path\" : \"https://example.com\" } } ] } and save it as /home/user/config.json","title":"Prepare JSON config"},{"location":"uk/advanced-docs/config-encryption/#prepare-key-for-encryption","text":"Use some of stored in src/utils/crypto.go.ENCRYPTION_KEYS or generate new one. For example lets use: some long password to encrypt config","title":"Prepare key for encryption"},{"location":"uk/advanced-docs/config-encryption/#encrypt-config","text":"","title":"Encrypt config"},{"location":"uk/advanced-docs/config-encryption/#using-make","text":"make DEFAULT_CONFIG = /home/user/config.json encrypt_config Enter passphrase ( leave empty to autogenerate a secure one ) : some long password to encrypt config Confirm passphrase: some long password to encrypt config Saved in file: /tmp/fileMx6JFo Save value as env variable: export DEFAULT_CONFIG_VALUE = 'YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IHNjcnlwdCAwS1pOUXlLM004L1NxcDRwL21CaUl3IDE4Cjc1cEZtcmZZZHJieFRvd0hhM0RVVVMxb2VMY0RmQzBkUkJQMXE2UWdyMUEKLS0tIHlSK1VFMkNOSHovbzRqUlJ3RW5VclphRy9TU0NQSG8vMzJUZ1c4RUozZncKD6zE4MONozWBfQYn9HG31DW100o2oFpn6iACQAvCDyXkgSeuQtRFjPwCIW5q2Dltq7Srkc8b81/ZynC59uqkmDJefGyNPzTk3ilRl6wcLOhCP1TD7YtCtZ/7ZpoGpNMiDD6XhKnOmz10sBSy1SXt54+zFVcuQ1ITRi4E2WmiFRjTa8T+ZMwurW+F+iwOu6+z8/0sKQaG5SrKA74GI9D6iRQnqiPg2Abr97Vq7X2Fjvz2NqFjcB0dD29XijHcLCdXQ1DcI3gx94SdMmmfeU5ub2ArsH/4nA8XlS7YE7BirUihgHD4/KIr52dc+Fst6i7SBH433d/Y3Pmhi89FHY8+sGyPFXNG+SeLLHafcR6bLLGyk0iGa2bZaBqUGovYNojni8KSrLRPXTgCyeNAOS7Gpamwi1Xco7m7nEEmAv9vpEvtOUx83pGBOkgu3oSV0t3jmp+OUvcwMMQ=' Encrypted config file saved to /tmp/fileMx6JFo file. And can be saved and distributed anywhere.","title":"Using make"},{"location":"uk/advanced-docs/config-encryption/#using-age","text":"age --encrypt -p --output = encrypted_config.json /home/user/config.json Enter passphrase ( leave empty to autogenerate a secure one ) : some long password to encrypt config Confirm passphrase: some long password to encrypt config","title":"Using age"},{"location":"uk/advanced-docs/config-encryption/#converting-to-base64","text":"cat encrypted_config.json | base64 | tr -d '\\n' YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IHNjcnlwdCAwS1pOUXlLM004L1NxcDRwL21CaUl3IDE4Cjc1cEZtcmZZZHJieFRvd0hhM0RVVVMxb2VMY0RmQzBkUkJQMXE2UWdyMUEKLS0tIHlSK1VFMkNOSHovbzRqUlJ3RW5VclphRy9TU0NQSG8vMzJUZ1c4RUozZncKD6zE4MONozWBfQYn9HG31DW100o2oFpn6iACQAvCDyXkgSeuQtRFjPwCIW5q2Dltq7Srkc8b81/ZynC59uqkmDJefGyNPzTk3ilRl6wcLOhCP1TD7YtCtZ/7ZpoGpNMiDD6XhKnOmz10sBSy1SXt54+zFVcuQ1ITRi4E2WmiFRjTa8T+ZMwurW+F+iwOu6+z8/0sKQaG5SrKA74GI9D6iRQnqiPg2Abr97Vq7X2Fjvz2NqFjcB0dD29XijHcLCdXQ1DcI3gx94SdMmmfeU5ub2ArsH/4nA8XlS7YE7BirUihgHD4/KIr52dc+Fst6i7SBH433d/Y3Pmhi89FHY8+sGyPFXNG+SeLLHafcR6bLLGyk0iGa2bZaBqUGovYNojni8KSrLRPXTgCyeNAOS7Gpamwi1Xco7m7nEEmAv9vpEvtOUx83pGBOkgu3oSV0t3jmp+OUvcwMMQ =","title":"Converting to base64"},{"location":"uk/advanced-docs/config-encryption/#encrypt-only-part-of-the-config","text":"You can encrypt a single job and add it to plaintext config (nothing stopping you from encrypting it further afterwards) via encrypted job entry: { \"jobs\" : [ { \"type\" : \"encrypted\" , \"args\" : { \"format\" : \"json\" , \"data\" : \"YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IHNjcnlwdCB5eCtiMzQ5RWlZRXo4dTNpRE8veHdRIDE4CmYyb2d0YXlnaXptS25sbUJlQUVaUHpRbngwaUdBYUpJRStHbFltdUVNNkUKLS0tIG5oUUVCd041TWJoNWNCQjhvODk4eUFpUldmUFUvaStpanRsdCtWR0RrSVkK2ehc+JYVl+f5VgLKV0mG/J4CQrtHn+FFV5AAcKiLEAjU6MNDaVqBI6Qm9RunLZ51wAA13DLZkPJH39DcsS77H3HmgLpRQ7DMFG2AIDxWysIt2Yi2hVVn9Ogea73twGa8FOpk2kk0Z7NSHCCcpTJd1Db4cwYJiIFaqfBXR+VZtNk3qBgUMStN1CiOyJxvHbnc6tbfeqq042LImKsaLvFzB2y5H/ec9BonHimrP/aZv6dhequs\" } }, { \"type\" : \"tcp\" , \"count\" : 100 , \"args\" : { \"address\" : \"localhost:9090\" , \"body\" : \"more_test\" , \"interval_ms\" : 1000 } } ] } Where args.format represents the encoding format you used for the data before encryption and args.data is a single ecrypted job. To encrypt a single job use same steps as to encrypt the whole config but use a file that contains just the job definition: { \"type\" : \"tcp\" , \"count\" : 100 , \"args\" : { \"address\" : \"localhost:9090\" , \"body\" : \"more_test\" , \"interval_ms\" : 1000 } } Note: each single decryption needs at least 256mb of RAM to set up an encryption key (this is implemented by age as hardening against bruteforce attempts). Due to that encrypting multiple jobs separately may be inefficient and it's advised to use parallel job to group multiple independed jobs into one: { \"type\" : \"parallel\" , \"args\" : { \"jobs\" : [ { \"type\" : \"tcp\" , \"count\" : 100 , \"args\" : { \"address\" : \"localhost:9090\" , \"body\" : \"payload\" , \"interval_ms\" : 1000 } }, { \"type\" : \"tcp\" , \"count\" : 100 , \"args\" : { \"address\" : \"localhost:9091\" , \"body\" : \"payload\" , \"interval_ms\" : 1000 } } ] } }","title":"Encrypt only part of the config"},{"location":"uk/advanced-docs/config-encryption/#embedding-encrypted-config-as-default-backup-config-into-binary","text":"Export env variable as it printed: export DEFAULT_CONFIG_VALUE = 'YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IHNjcnlwdCAwS1pOUXlLM004L1NxcDRwL21CaUl3IDE4Cjc1cEZtcmZZZHJieFRvd0hhM0RVVVMxb2VMY0RmQzBkUkJQMXE2UWdyMUEKLS0tIHlSK1VFMkNOSHovbzRqUlJ3RW5VclphRy9TU0NQSG8vMzJUZ1c4RUozZncKD6zE4MONozWBfQYn9HG31DW100o2oFpn6iACQAvCDyXkgSeuQtRFjPwCIW5q2Dltq7Srkc8b81/ZynC59uqkmDJefGyNPzTk3ilRl6wcLOhCP1TD7YtCtZ/7ZpoGpNMiDD6XhKnOmz10sBSy1SXt54+zFVcuQ1ITRi4E2WmiFRjTa8T+ZMwurW+F+iwOu6+z8/0sKQaG5SrKA74GI9D6iRQnqiPg2Abr97Vq7X2Fjvz2NqFjcB0dD29XijHcLCdXQ1DcI3gx94SdMmmfeU5ub2ArsH/4nA8XlS7YE7BirUihgHD4/KIr52dc+Fst6i7SBH433d/Y3Pmhi89FHY8+sGyPFXNG+SeLLHafcR6bLLGyk0iGa2bZaBqUGovYNojni8KSrLRPXTgCyeNAOS7Gpamwi1Xco7m7nEEmAv9vpEvtOUx83pGBOkgu3oSV0t3jmp+OUvcwMMQ=' This base64 value is encrypted config with specified password (hashed with scrypt against brute force). Build new binary with new config: make build_encrypted GOOS = linux GOARCH = amd64 CGO_ENABLED = 0 go build -ldflags = \"-X 'github.com/Arriven/db1000n/src/job/config.DefaultConfig=YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IHNjcnlwdCAwS1pOUXlLM004L1NxcDRwL21CaUl3IDE4Cjc1cEZtcmZZZHJieFRvd0hhM0RVVVMxb2VMY0RmQzBkUkJQMXE2UWdyMUEKLS0tIHlSK1VFMkNOSHovbzRqUlJ3RW5VclphRy9TU0NQSG8vMzJUZ1c4RUozZncKD6zE4MONozWBfQYn9HG31DW100o2oFpn6iACQAvCDyXkgSeuQtRFjPwCIW5q2Dltq7Srkc8b81/ZynC59uqkmDJefGyNPzTk3ilRl6wcLOhCP1TD7YtCtZ/7ZpoGpNMiDD6XhKnOmz10sBSy1SXt54+zFVcuQ1ITRi4E2WmiFRjTa8T+ZMwurW+F+iwOu6+z8/0sKQaG5SrKA74GI9D6iRQnqiPg2Abr97Vq7X2Fjvz2NqFjcB0dD29XijHcLCdXQ1DcI3gx94SdMmmfeU5ub2ArsH/4nA8XlS7YE7BirUihgHD4/KIr52dc+Fst6i7SBH433d/Y3Pmhi89FHY8+sGyPFXNG+SeLLHafcR6bLLGyk0iGa2bZaBqUGovYNojni8KSrLRPXTgCyeNAOS7Gpamwi1Xco7m7nEEmAv9vpEvtOUx83pGBOkgu3oSV0t3jmp+OUvcwMMQ='\" -o main ./main.go Your new binary saved as main with new encrypted and embedded default config. To turn on decryption new config you should pass encryption keys as list of keys separated with & symbol: export ENCRYPTION_KEYS = 'some long password to encrypt config&another key' It will override default encryption keys","title":"Embedding encrypted config as default backup config into binary"},{"location":"uk/advanced-docs/config-encryption/#embedding-encryption-keys-into-binary","text":"You can embed keys into binary in same way: export ENCRYPTION_KEYS = 'some long password to encrypt config&another key' make build_encrypted GOOS = linux GOARCH = amd64 CGO_ENABLED = 0 go build -ldflags = \"-X 'github.com/Arriven/db1000n/src/utils.EncryptionKeys=some long password to encrypt config&another key' -X 'github.com/Arriven/db1000n/src/job/config.DefaultConfig=YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IHNjcnlwdCAwS1pOUXlLM004L1NxcDRwL21CaUl3IDE4Cjc1cEZtcmZZZHJieFRvd0hhM0RVVVMxb2VMY0RmQzBkUkJQMXE2UWdyMUEKLS0tIHlSK1VFMkNOSHovbzRqUlJ3RW5VclphRy9TU0NQSG8vMzJUZ1c4RUozZncKD6zE4MONozWBfQYn9HG31DW100o2oFpn6iACQAvCDyXkgSeuQtRFjPwCIW5q2Dltq7Srkc8b81/ZynC59uqkmDJefGyNPzTk3ilRl6wcLOhCP1TD7YtCtZ/7ZpoGpNMiDD6XhKnOmz10sBSy1SXt54+zFVcuQ1ITRi4E2WmiFRjTa8T+ZMwurW+F+iwOu6+z8/0sKQaG5SrKA74GI9D6iRQnqiPg2Abr97Vq7X2Fjvz2NqFjcB0dD29XijHcLCdXQ1DcI3gx94SdMmmfeU5ub2ArsH/4nA8XlS7YE7BirUihgHD4/KIr52dc+Fst6i7SBH433d/Y3Pmhi89FHY8+sGyPFXNG+SeLLHafcR6bLLGyk0iGa2bZaBqUGovYNojni8KSrLRPXTgCyeNAOS7Gpamwi1Xco7m7nEEmAv9vpEvtOUx83pGBOkgu3oSV0t3jmp+OUvcwMMQ='\" -o main ./main.go","title":"Embedding encryption keys into binary"},{"location":"uk/advanced-docs/configuration/","text":"Configuration \u00b6 Command Line reference \u00b6 Usage of db1000n: -b string raw backup config in case the primary one is unavailable -c string path to config files, separated by a comma, each path can be a web endpoint (default \"https://raw.githubusercontent.com/db1000n-coordinators/LoadTestConfig/main/config.v0.7.json\") -country-list string comma-separated list of countries (default \"Ukraine\") -debug enable debug level logging -enable-primitive set to true if you want to run primitive jobs that are less resource-efficient (default true) -enable-self-update Enable the application automatic updates on the startup -format string config format (default \"yaml\") -h print help message and exit -pprof string enable pprof -prometheus_gateways string Comma separated list of prometheus push gateways (default \"https://178.62.78.144:9091,https://46.101.26.43:9091,https://178.62.33.149:9091\") -prometheus_on Start metrics exporting via HTTP and pushing to gateways (specified via <prometheus_gateways>) (default true) -proxy string system proxy to set by default (can be a comma-separated list or a template) -refresh-interval duration refresh timeout for updating the config (default 1m0s) -restart-on-update Allows application to restart upon successful update (ignored if auto-update is disabled) (default true) -scale int used to scale the amount of jobs being launched, effect is similar to launching multiple instances at once (default 1) -self-update-check-frequency duration How often to run auto-update checks (default 24h0m0s) -skip-encrypted set to true if you want to only run plaintext jobs from the config for security considerations -skip-update-check-on-start Allows to skip the update check at the startup (usually set automatically by the previous version) -strict-country-check enable strict country check; will also exit if IP can't be determined -updater-destination-config string Destination config file to write (only applies if updater-mode is enabled (default \"config/config.json\") -updater-mode Only run config updater Almost all of these parameters can also be set via environment variables Config file reference \u00b6 This doc gets outdated frequently as the project is under active development but you can always check up to date configuration examples in examples/config folder The config is expected to be in json format and has following configuration values: jobs - [array] array of attack job definitions to run, should be defined inside the root object jobs[*] - [object] single job definition as json object jobs[*].type - [string] type of the job (determines which attack function to launch). Can be http , tcp , udp , syn-flood , or packetgen jobs[*].count - [number] the amount of instances of the job to be launched, automatically set to 1 if no or invalid value is specified jobs[*].args - [object] arguments to pass to the job. Depends on jobs[*].type http args: request - [object] defines requests to be sent request.method - [string] http method to use (passed directly to go http.NewRequest ) request.path - [string] url path to use (passed directly to go http.NewRequest ) request.body - [object] http payload to use (passed directly to go http.NewRequest ) request.headers - [object] key-value map of http headers request.cookies - [object] key-value map of http cookies (you can still set cookies directly via the header with cookie_string template function or statically, see examples/config/advanced/ddos-guard.yaml for an example) client - [object] http client config for the job client.tls_config - [object] tls config for transport (InsecureSkipVerify is true by default) client.proxy_urls - [array] comma-separated list of string urls for proxies to use (chosen randomly for each request) client.timeout - [time.Duration] client.max_idle_connections - [number] tcp args: proxy_urls - [string] comma-separated list of http/socks5 proxies to use (chosen randomly for each job) timeout - [duration] timeout for connecting to the proxy tcp and udp shared args: address - [string] network host to connect to, can be either hostname:port or ip:port body - [object] json data to be repeatedly sent over the network Warning: packetgen requires root privileges to run packetgen args: connection - [object] raw ip connection parameters connection.name - [string] name of the network to use. can be ip4:tcp , ip6:tcp , ip4:udp , ip6:udp , or anything else supported by the go runtime connection.address - [string] address of the interface used to send packets (on the attacking machine) packet - [object] packet configuration parameters. see examples/config/advanced/packetgen-* for usage examples as there are just too many params to put them here. I'll only describe the general structure of the packet packet.link - [layer] tcp/ip level 1 (OSI level 2) configuration. currently only supports ethernet serialization but go runtime doesn't have a way to send custom ethernet frames so it's not advised to use it packet.network - [layer] tcp/ip level 2 (OSI level 3) configuration. supports ipv4 and ipv6 protocols. see src/core/packetgen/network.go for all the available options packet.transport - [layer] tcp/ip level 3 (OSI level 4) configuration. supports tcp and udp protocols. see src/core/packetgen/transport.go for all the available options packet.payload - [layer] the data that goes on top of other layers. for now it can be raw for custom crafted payload string (i.e. you can write an http request directly here), dns , and icmpv4 , but last two are not fully tested yet all the jobs have shared args: interval_ms - [number] interval between requests in milliseconds. Defaults to 0 (Care, in case of udp job it might generate the data faster than your OS/network card can process it) count - [number] limit the amount of requests to send with this job invocation. Defaults to 0 (no limit). Note: if config is refreshed before this limit is reached the job will be restarted and the counter will be reset Almost every leaf [string] or [object] parameter can be templated with go template syntax. I've also added couple helper functions (list will be growing): random_uuid random_int_n\" random_int random_payload random_ip random_port random_mac_addr random_user_agent local_ip local_ipv4 local_ipv6 local_mac_addr resolve_host resolve_host_ipv4 resolve_host_ipv6 base64_encode base64_decode to_yaml from_yaml from_yaml_array to_json from_json from_json_array from_string_array join split get_url mod ctx_key split cookie_string Please refer to official go documentation and code in src/utils/templates/ for these for now","title":"Configuration"},{"location":"uk/advanced-docs/configuration/#configuration","text":"","title":"Configuration"},{"location":"uk/advanced-docs/configuration/#command-line-reference","text":"Usage of db1000n: -b string raw backup config in case the primary one is unavailable -c string path to config files, separated by a comma, each path can be a web endpoint (default \"https://raw.githubusercontent.com/db1000n-coordinators/LoadTestConfig/main/config.v0.7.json\") -country-list string comma-separated list of countries (default \"Ukraine\") -debug enable debug level logging -enable-primitive set to true if you want to run primitive jobs that are less resource-efficient (default true) -enable-self-update Enable the application automatic updates on the startup -format string config format (default \"yaml\") -h print help message and exit -pprof string enable pprof -prometheus_gateways string Comma separated list of prometheus push gateways (default \"https://178.62.78.144:9091,https://46.101.26.43:9091,https://178.62.33.149:9091\") -prometheus_on Start metrics exporting via HTTP and pushing to gateways (specified via <prometheus_gateways>) (default true) -proxy string system proxy to set by default (can be a comma-separated list or a template) -refresh-interval duration refresh timeout for updating the config (default 1m0s) -restart-on-update Allows application to restart upon successful update (ignored if auto-update is disabled) (default true) -scale int used to scale the amount of jobs being launched, effect is similar to launching multiple instances at once (default 1) -self-update-check-frequency duration How often to run auto-update checks (default 24h0m0s) -skip-encrypted set to true if you want to only run plaintext jobs from the config for security considerations -skip-update-check-on-start Allows to skip the update check at the startup (usually set automatically by the previous version) -strict-country-check enable strict country check; will also exit if IP can't be determined -updater-destination-config string Destination config file to write (only applies if updater-mode is enabled (default \"config/config.json\") -updater-mode Only run config updater Almost all of these parameters can also be set via environment variables","title":"Command Line reference"},{"location":"uk/advanced-docs/configuration/#config-file-reference","text":"This doc gets outdated frequently as the project is under active development but you can always check up to date configuration examples in examples/config folder The config is expected to be in json format and has following configuration values: jobs - [array] array of attack job definitions to run, should be defined inside the root object jobs[*] - [object] single job definition as json object jobs[*].type - [string] type of the job (determines which attack function to launch). Can be http , tcp , udp , syn-flood , or packetgen jobs[*].count - [number] the amount of instances of the job to be launched, automatically set to 1 if no or invalid value is specified jobs[*].args - [object] arguments to pass to the job. Depends on jobs[*].type http args: request - [object] defines requests to be sent request.method - [string] http method to use (passed directly to go http.NewRequest ) request.path - [string] url path to use (passed directly to go http.NewRequest ) request.body - [object] http payload to use (passed directly to go http.NewRequest ) request.headers - [object] key-value map of http headers request.cookies - [object] key-value map of http cookies (you can still set cookies directly via the header with cookie_string template function or statically, see examples/config/advanced/ddos-guard.yaml for an example) client - [object] http client config for the job client.tls_config - [object] tls config for transport (InsecureSkipVerify is true by default) client.proxy_urls - [array] comma-separated list of string urls for proxies to use (chosen randomly for each request) client.timeout - [time.Duration] client.max_idle_connections - [number] tcp args: proxy_urls - [string] comma-separated list of http/socks5 proxies to use (chosen randomly for each job) timeout - [duration] timeout for connecting to the proxy tcp and udp shared args: address - [string] network host to connect to, can be either hostname:port or ip:port body - [object] json data to be repeatedly sent over the network Warning: packetgen requires root privileges to run packetgen args: connection - [object] raw ip connection parameters connection.name - [string] name of the network to use. can be ip4:tcp , ip6:tcp , ip4:udp , ip6:udp , or anything else supported by the go runtime connection.address - [string] address of the interface used to send packets (on the attacking machine) packet - [object] packet configuration parameters. see examples/config/advanced/packetgen-* for usage examples as there are just too many params to put them here. I'll only describe the general structure of the packet packet.link - [layer] tcp/ip level 1 (OSI level 2) configuration. currently only supports ethernet serialization but go runtime doesn't have a way to send custom ethernet frames so it's not advised to use it packet.network - [layer] tcp/ip level 2 (OSI level 3) configuration. supports ipv4 and ipv6 protocols. see src/core/packetgen/network.go for all the available options packet.transport - [layer] tcp/ip level 3 (OSI level 4) configuration. supports tcp and udp protocols. see src/core/packetgen/transport.go for all the available options packet.payload - [layer] the data that goes on top of other layers. for now it can be raw for custom crafted payload string (i.e. you can write an http request directly here), dns , and icmpv4 , but last two are not fully tested yet all the jobs have shared args: interval_ms - [number] interval between requests in milliseconds. Defaults to 0 (Care, in case of udp job it might generate the data faster than your OS/network card can process it) count - [number] limit the amount of requests to send with this job invocation. Defaults to 0 (no limit). Note: if config is refreshed before this limit is reached the job will be restarted and the counter will be reset Almost every leaf [string] or [object] parameter can be templated with go template syntax. I've also added couple helper functions (list will be growing): random_uuid random_int_n\" random_int random_payload random_ip random_port random_mac_addr random_user_agent local_ip local_ipv4 local_ipv6 local_mac_addr resolve_host resolve_host_ipv4 resolve_host_ipv6 base64_encode base64_decode to_yaml from_yaml from_yaml_array to_json from_json from_json_array from_string_array join split get_url mod ctx_key split cookie_string Please refer to official go documentation and code in src/utils/templates/ for these for now","title":"Config file reference"},{"location":"uk/advanced-docs/docker-vpn/","text":"Docker VPN \u00b6 Setting up VPN for Docker users \u00b6 In case of using a dedicated VPS that has banned public IP, a container with OpenVPN client can be deployed inside the same network as db1000n is in. One of the easy ways to set it up is through the docker-compose. There are few docker-compose examples, see examples/docker . Documentation you can find below: Static Docker Compose \u00b6 openvpn/auth.txt : <your username for OpenVPN> <your password for OpenVPN> Also place your *.ovpn file into openvpn/ directory. You can set multiple configuration files and one of them will be used. Old Docker Compose \u00b6 openvpn/provider01.txt : <your username for OpenVPN provider 01> <your password for OpenVPN provider 01> openvpn/provider02.txt : <your username for OpenVPN provider 02> <your password for OpenVPN provider 02> Also place your provider01.endpoint01.conf , provider01.endpoint02.conf and provider02.endpoint01.conf files into openvpn/ directory. Start \u00b6 docker-compose -f examples/docker/your_docker_file.yml up -d Stop \u00b6 docker-compose -f examples/docker/your_docker_file.yml down","title":"Docker VPN"},{"location":"uk/advanced-docs/docker-vpn/#docker-vpn","text":"","title":"Docker VPN"},{"location":"uk/advanced-docs/docker-vpn/#setting-up-vpn-for-docker-users","text":"In case of using a dedicated VPS that has banned public IP, a container with OpenVPN client can be deployed inside the same network as db1000n is in. One of the easy ways to set it up is through the docker-compose. There are few docker-compose examples, see examples/docker . Documentation you can find below:","title":"Setting up VPN for Docker users"},{"location":"uk/advanced-docs/docker-vpn/#static-docker-compose","text":"openvpn/auth.txt : <your username for OpenVPN> <your password for OpenVPN> Also place your *.ovpn file into openvpn/ directory. You can set multiple configuration files and one of them will be used.","title":"Static Docker Compose"},{"location":"uk/advanced-docs/docker-vpn/#old-docker-compose","text":"openvpn/provider01.txt : <your username for OpenVPN provider 01> <your password for OpenVPN provider 01> openvpn/provider02.txt : <your username for OpenVPN provider 02> <your password for OpenVPN provider 02> Also place your provider01.endpoint01.conf , provider01.endpoint02.conf and provider02.endpoint01.conf files into openvpn/ directory.","title":"Old Docker Compose"},{"location":"uk/advanced-docs/docker-vpn/#start","text":"docker-compose -f examples/docker/your_docker_file.yml up -d","title":"Start"},{"location":"uk/advanced-docs/docker-vpn/#stop","text":"docker-compose -f examples/docker/your_docker_file.yml down","title":"Stop"},{"location":"uk/advanced-docs/pull-request-template/","text":"Pull request template \u00b6 Please include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change. Fixes # (issue) Type of change \u00b6 Please delete options that are not relevant. [ ] Bug fix (non-breaking change which fixes an issue) [ ] New feature (non-breaking change which adds functionality) [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected) [ ] Documentation update How Has This Been Tested? \u00b6 Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration [ ] Test A [ ] Test B Test Configuration \u00b6 Release version: Platform: Logs \u00b6 logs Screenshots \u00b6 [ ] No screenshot","title":"Pull request template"},{"location":"uk/advanced-docs/pull-request-template/#pull-request-template","text":"Please include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change. Fixes # (issue)","title":"Pull request template"},{"location":"uk/advanced-docs/pull-request-template/#type-of-change","text":"Please delete options that are not relevant. [ ] Bug fix (non-breaking change which fixes an issue) [ ] New feature (non-breaking change which adds functionality) [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected) [ ] Documentation update","title":"Type of change"},{"location":"uk/advanced-docs/pull-request-template/#how-has-this-been-tested","text":"Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration [ ] Test A [ ] Test B","title":"How Has This Been Tested?"},{"location":"uk/advanced-docs/pull-request-template/#test-configuration","text":"Release version: Platform:","title":"Test Configuration"},{"location":"uk/advanced-docs/pull-request-template/#logs","text":"logs","title":"Logs"},{"location":"uk/advanced-docs/pull-request-template/#screenshots","text":"[ ] No screenshot","title":"Screenshots"},{"location":"uk/advanced-docs/kubernetes/helm-charts/","text":"Helm charts \u00b6 If you want to use plain manifests, see Manifests \u00b6 This is a Helm chart for Kubernetes Prerequisites \u00b6 Make sure that you installed helm package on your local machine and you have connection to the Kubernetes cluster. Install a release \u00b6 cd kubernetes/helm-charts/ helm upgrade --install \\ --create-namespace \\ --namespace = db1000n \\ -f values.yaml db1000n . Destroy a release \u00b6 helm uninstall db1000n -n db1000n","title":"Helm charts"},{"location":"uk/advanced-docs/kubernetes/helm-charts/#helm-charts","text":"","title":"Helm charts"},{"location":"uk/advanced-docs/kubernetes/helm-charts/#if-you-want-to-use-plain-manifests-see-manifests","text":"This is a Helm chart for Kubernetes","title":"If you want to use plain manifests, see Manifests"},{"location":"uk/advanced-docs/kubernetes/helm-charts/#prerequisites","text":"Make sure that you installed helm package on your local machine and you have connection to the Kubernetes cluster.","title":"Prerequisites"},{"location":"uk/advanced-docs/kubernetes/helm-charts/#install-a-release","text":"cd kubernetes/helm-charts/ helm upgrade --install \\ --create-namespace \\ --namespace = db1000n \\ -f values.yaml db1000n .","title":"Install a release"},{"location":"uk/advanced-docs/kubernetes/helm-charts/#destroy-a-release","text":"helm uninstall db1000n -n db1000n","title":"Destroy a release"},{"location":"uk/advanced-docs/kubernetes/manifests/","text":"Manifests install \u00b6 If you use Helm, see our Helm Chart \u00b6 There are two ways to deploy it with plain manifests: using Deployment using DaemonSet Deployment \u00b6 Install: cd kubernetes/manifests/ kubectl apply -f deployment.yaml kubectl get po -n db1000n Scale: kubectl scale deployment/db1000n --replicas = 10 -n db1000n Destroy: kubectl delete deploy db1000n -n db1000n DaemonSet \u00b6 Get and label nodes where you need to run db1000n . There should be nodes at least with 2CPU and 2GB of RAM, CPU resources in priority for db1000n : kubectl get nodes Select nodes where you want to run db1000n from the output and label them: kubectl label nodes <YOUR_UNIQUE_NODE_NAME> db1000n = true Install the DaemonSet: kubectl apply -f daemonset.yaml Destroy: kubectl delete daemonset db1000n -n db1000n How it works? DaemonSet will create one db1000n pod on each node that labeled as db1000n=true . It coule be useful in large cluster types that can be autoscaled horizontally, for example, GKE standard k8s cluster from the free tier purposes.","title":"Manifests install"},{"location":"uk/advanced-docs/kubernetes/manifests/#manifests-install","text":"","title":"Manifests install"},{"location":"uk/advanced-docs/kubernetes/manifests/#if-you-use-helm-see-our-helm-chart","text":"There are two ways to deploy it with plain manifests: using Deployment using DaemonSet","title":"If you use Helm, see our Helm Chart"},{"location":"uk/advanced-docs/kubernetes/manifests/#deployment","text":"Install: cd kubernetes/manifests/ kubectl apply -f deployment.yaml kubectl get po -n db1000n Scale: kubectl scale deployment/db1000n --replicas = 10 -n db1000n Destroy: kubectl delete deploy db1000n -n db1000n","title":"Deployment"},{"location":"uk/advanced-docs/kubernetes/manifests/#daemonset","text":"Get and label nodes where you need to run db1000n . There should be nodes at least with 2CPU and 2GB of RAM, CPU resources in priority for db1000n : kubectl get nodes Select nodes where you want to run db1000n from the output and label them: kubectl label nodes <YOUR_UNIQUE_NODE_NAME> db1000n = true Install the DaemonSet: kubectl apply -f daemonset.yaml Destroy: kubectl delete daemonset db1000n -n db1000n How it works? DaemonSet will create one db1000n pod on each node that labeled as db1000n=true . It coule be useful in large cluster types that can be autoscaled horizontally, for example, GKE standard k8s cluster from the free tier purposes.","title":"DaemonSet"},{"location":"uk/advanced-docs/terraform/aws_ec2/","text":"AWS EC2 \u00b6 Requirements \u00b6 AWS account terraform installed Deploy \u00b6 To deploy run: terraform apply -var-file = \"ireland.tfvars\" You can create new *.tfvars files for different regions and accounts. To swich between regions you can use terraform workspace command. For example: terraform init terraform workspace new eu terraform apply -var-file = \"ireland.tfvars\" terraform workspace new us terraform apply -var-file = \"useast.tfvars\" Destroy \u00b6 To destroy infrastructure use commands: terraform workspace select eu terraform destroy -var-file = \"ireland.tfvars\" terraform workspace select us terraform destroy -var-file = \"useast.tfvars\"","title":"AWS EC2"},{"location":"uk/advanced-docs/terraform/aws_ec2/#aws-ec2","text":"","title":"AWS EC2"},{"location":"uk/advanced-docs/terraform/aws_ec2/#requirements","text":"AWS account terraform installed","title":"Requirements"},{"location":"uk/advanced-docs/terraform/aws_ec2/#deploy","text":"To deploy run: terraform apply -var-file = \"ireland.tfvars\" You can create new *.tfvars files for different regions and accounts. To swich between regions you can use terraform workspace command. For example: terraform init terraform workspace new eu terraform apply -var-file = \"ireland.tfvars\" terraform workspace new us terraform apply -var-file = \"useast.tfvars\"","title":"Deploy"},{"location":"uk/advanced-docs/terraform/aws_ec2/#destroy","text":"To destroy infrastructure use commands: terraform workspace select eu terraform destroy -var-file = \"ireland.tfvars\" terraform workspace select us terraform destroy -var-file = \"useast.tfvars\"","title":"Destroy"},{"location":"uk/advanced-docs/terraform/aws_eks/","text":"AWS EKS \u00b6 Description \u00b6 This implementation allows you to create entire AWS infrastructure from scratch and provides Kubernetes cluster (EKS) to deploy db1000n project. Prerequisites \u00b6 AWS account with AdministratorAccess permissions OS Linux or Windows AWS CLI Terraform Helm kubectl Configure AWS profile \u00b6 The following example shows sample values: $ aws configure AWS Access Key ID [ None ] : AKIAIOSFODNN7EXAMPLE AWS Secret Access Key [ None ] : wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY Default region name [ None ] : us-west-2 Default output format [ None ] : json Deployment \u00b6 Deploy infrastructure \u00b6 cd db1000n/terraform/aws/eks-cluster/ terraform init terraform plan terraform apply NOTE: You can create multilpe *.tfvars configuration files with various variables, regions and AWS accounts using terraform workspace command: cd db1000n/terraform/aws/eks-cluster/ terraform init terraform workspace new $your_workspace terraform plan -var-file $your_file .tfvars terraform apply -var-file $your_file .tfvars Update kubeconfig \u00b6 aws --profile $your_aws_profile eks update-kubeconfig --name $your_eks_cluster_name Connect to EKS cluster \u00b6 $ kubectl get nodes NAME STATUS ROLES AGE VERSION ip-xxx-xxx-x-xx.us-east-1.compute.internal Ready <none> 107m v1.21.5-eks-9017834 ip-xxx-xxx-x-xx.us-east-1.compute.internal Ready <none> 107m v1.21.5-eks-9017834 ip-xxx-xxx-x-xx.us-east-1.compute.internal Ready <none> 107m v1.21.5-eks-9017834 Install application \u00b6 $ cd db1000n/kubernetes/helm-charts/ $ helm upgrade --install \\ --create-namespace \\ --namespace = db1000n \\ -f values.yaml db1000n . Check installation \u00b6 $ kubectl -n db1000n get pods NAME READY STATUS RESTARTS AGE db1000n-54d8744b54-8hffr 1 /1 Running 0 2m10s db1000n-54d8744b54-8vml4 1 /1 Running 0 2m10s db1000n-54d8744b54-9stzv 1 /1 Running 0 2m10s Deletion \u00b6 Delete application \u00b6 helm uninstall db1000n -n db1000n Delete infrastructure \u00b6 terraform destroy","title":"AWS EKS"},{"location":"uk/advanced-docs/terraform/aws_eks/#aws-eks","text":"","title":"AWS EKS"},{"location":"uk/advanced-docs/terraform/aws_eks/#description","text":"This implementation allows you to create entire AWS infrastructure from scratch and provides Kubernetes cluster (EKS) to deploy db1000n project.","title":"Description"},{"location":"uk/advanced-docs/terraform/aws_eks/#prerequisites","text":"AWS account with AdministratorAccess permissions OS Linux or Windows AWS CLI Terraform Helm kubectl","title":"Prerequisites"},{"location":"uk/advanced-docs/terraform/aws_eks/#configure-aws-profile","text":"The following example shows sample values: $ aws configure AWS Access Key ID [ None ] : AKIAIOSFODNN7EXAMPLE AWS Secret Access Key [ None ] : wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY Default region name [ None ] : us-west-2 Default output format [ None ] : json","title":"Configure AWS profile"},{"location":"uk/advanced-docs/terraform/aws_eks/#deployment","text":"","title":"Deployment"},{"location":"uk/advanced-docs/terraform/aws_eks/#deploy-infrastructure","text":"cd db1000n/terraform/aws/eks-cluster/ terraform init terraform plan terraform apply NOTE: You can create multilpe *.tfvars configuration files with various variables, regions and AWS accounts using terraform workspace command: cd db1000n/terraform/aws/eks-cluster/ terraform init terraform workspace new $your_workspace terraform plan -var-file $your_file .tfvars terraform apply -var-file $your_file .tfvars","title":"Deploy infrastructure"},{"location":"uk/advanced-docs/terraform/aws_eks/#update-kubeconfig","text":"aws --profile $your_aws_profile eks update-kubeconfig --name $your_eks_cluster_name","title":"Update kubeconfig"},{"location":"uk/advanced-docs/terraform/aws_eks/#connect-to-eks-cluster","text":"$ kubectl get nodes NAME STATUS ROLES AGE VERSION ip-xxx-xxx-x-xx.us-east-1.compute.internal Ready <none> 107m v1.21.5-eks-9017834 ip-xxx-xxx-x-xx.us-east-1.compute.internal Ready <none> 107m v1.21.5-eks-9017834 ip-xxx-xxx-x-xx.us-east-1.compute.internal Ready <none> 107m v1.21.5-eks-9017834","title":"Connect to EKS cluster"},{"location":"uk/advanced-docs/terraform/aws_eks/#install-application","text":"$ cd db1000n/kubernetes/helm-charts/ $ helm upgrade --install \\ --create-namespace \\ --namespace = db1000n \\ -f values.yaml db1000n .","title":"Install application"},{"location":"uk/advanced-docs/terraform/aws_eks/#check-installation","text":"$ kubectl -n db1000n get pods NAME READY STATUS RESTARTS AGE db1000n-54d8744b54-8hffr 1 /1 Running 0 2m10s db1000n-54d8744b54-8vml4 1 /1 Running 0 2m10s db1000n-54d8744b54-9stzv 1 /1 Running 0 2m10s","title":"Check installation"},{"location":"uk/advanced-docs/terraform/aws_eks/#deletion","text":"","title":"Deletion"},{"location":"uk/advanced-docs/terraform/aws_eks/#delete-application","text":"helm uninstall db1000n -n db1000n","title":"Delete application"},{"location":"uk/advanced-docs/terraform/aws_eks/#delete-infrastructure","text":"terraform destroy","title":"Delete infrastructure"},{"location":"uk/advanced-docs/terraform/aws_lightsail/","text":"AWS Lightsail \u00b6 Requirements \u00b6 AWS account terraform (1.0+) installed Deploy \u00b6 To deploy: terraform init terraform plan terraform apply Destroy \u00b6 To destroy: terraform destroy","title":"AWS Lightsail"},{"location":"uk/advanced-docs/terraform/aws_lightsail/#aws-lightsail","text":"","title":"AWS Lightsail"},{"location":"uk/advanced-docs/terraform/aws_lightsail/#requirements","text":"AWS account terraform (1.0+) installed","title":"Requirements"},{"location":"uk/advanced-docs/terraform/aws_lightsail/#deploy","text":"To deploy: terraform init terraform plan terraform apply","title":"Deploy"},{"location":"uk/advanced-docs/terraform/aws_lightsail/#destroy","text":"To destroy: terraform destroy","title":"Destroy"},{"location":"uk/advanced-docs/terraform/azure/","text":"Azure \u00b6 Prerequisites \u00b6 Install terraform Have Azure account. Prepare environment for Azure Provider The easiest option for auth is Azure CLI Deployment \u00b6 The composition creates container instances in 6 different regions for a broader attack simulation. If you want to make a different setup, just alter modules in the main.tf . Create a new terraform.tfvars file in the folder, if you want to change the default configuration of the farm ( db1000n can be configured with either command line parameters or environment variables, former having precedence over the latter): bomblet_count=10 - can be used for custom number of containers per region attack_commands=[\"/usr/src/app/db1000n\",\"-c=https://link_to_your_config_file\"] attack_environment_variables={\"ENABLE_PRIMITIVE\":\"false\"} terraform init - to restore all dependencies. terraform apply -auto-approve - to provision the attack farm. Collecting logs from the containers \u00b6 The container instances are provisioned without public IP addresses to make the setup more cost effective. If you deploy more than one container per region, play with the -01 suffix to get logs from the correct instance. Logs from North Europe region: az container logs --resource-group main-rg --name main-northeurope-01 --container-name main Logs from West Europe region: az container logs --resource-group main-rg --name main-westeurope-01 --container-name main Logs from Canada Central region: az container logs --resource-group main-rg --name main-canadacentral-01 --container-name main Logs from UAE North region: az container logs --resource-group main-rg --name main-uaenorth-01 --container-name main Logs from Central US region: az container logs --resource-group main-rg --name main-centralus-01 --container-name main Logs from East Asia region: az container logs --resource-group main-rg --name main-eastasia-01 --container-name main Cleanup \u00b6 terraform destroy","title":"Azure"},{"location":"uk/advanced-docs/terraform/azure/#azure","text":"","title":"Azure"},{"location":"uk/advanced-docs/terraform/azure/#prerequisites","text":"Install terraform Have Azure account. Prepare environment for Azure Provider The easiest option for auth is Azure CLI","title":"Prerequisites"},{"location":"uk/advanced-docs/terraform/azure/#deployment","text":"The composition creates container instances in 6 different regions for a broader attack simulation. If you want to make a different setup, just alter modules in the main.tf . Create a new terraform.tfvars file in the folder, if you want to change the default configuration of the farm ( db1000n can be configured with either command line parameters or environment variables, former having precedence over the latter): bomblet_count=10 - can be used for custom number of containers per region attack_commands=[\"/usr/src/app/db1000n\",\"-c=https://link_to_your_config_file\"] attack_environment_variables={\"ENABLE_PRIMITIVE\":\"false\"} terraform init - to restore all dependencies. terraform apply -auto-approve - to provision the attack farm.","title":"Deployment"},{"location":"uk/advanced-docs/terraform/azure/#collecting-logs-from-the-containers","text":"The container instances are provisioned without public IP addresses to make the setup more cost effective. If you deploy more than one container per region, play with the -01 suffix to get logs from the correct instance. Logs from North Europe region: az container logs --resource-group main-rg --name main-northeurope-01 --container-name main Logs from West Europe region: az container logs --resource-group main-rg --name main-westeurope-01 --container-name main Logs from Canada Central region: az container logs --resource-group main-rg --name main-canadacentral-01 --container-name main Logs from UAE North region: az container logs --resource-group main-rg --name main-uaenorth-01 --container-name main Logs from Central US region: az container logs --resource-group main-rg --name main-centralus-01 --container-name main Logs from East Asia region: az container logs --resource-group main-rg --name main-eastasia-01 --container-name main","title":"Collecting logs from the containers"},{"location":"uk/advanced-docs/terraform/azure/#cleanup","text":"terraform destroy","title":"Cleanup"},{"location":"uk/advanced-docs/terraform/digital-ocean/","text":"Digital Ocean \u00b6 Requirements \u00b6 Digital Ocean account API token (Go to API - Personal access tokens) and generate Personal access token (with write permissions) terraform (1.0+) installed Deploy \u00b6 To deploy: export DO_TOKEN = <place-api-token-here> terraform init terraform plan -var \"do_token= ${ DO_TOKEN } \" terraform apply -var \"do_token= ${ DO_TOKEN } \" After deployment (usually takes 5-10 mins) go to Apps List , find an app with name db1000n and chek Runtime Logs. Destroy \u00b6 To destroy: export DO_TOKEN = <place-api-token-here> terraform destroy -var \"do_token= ${ DO_TOKEN } \"","title":"Digital Ocean"},{"location":"uk/advanced-docs/terraform/digital-ocean/#digital-ocean","text":"","title":"Digital Ocean"},{"location":"uk/advanced-docs/terraform/digital-ocean/#requirements","text":"Digital Ocean account API token (Go to API - Personal access tokens) and generate Personal access token (with write permissions) terraform (1.0+) installed","title":"Requirements"},{"location":"uk/advanced-docs/terraform/digital-ocean/#deploy","text":"To deploy: export DO_TOKEN = <place-api-token-here> terraform init terraform plan -var \"do_token= ${ DO_TOKEN } \" terraform apply -var \"do_token= ${ DO_TOKEN } \" After deployment (usually takes 5-10 mins) go to Apps List , find an app with name db1000n and chek Runtime Logs.","title":"Deploy"},{"location":"uk/advanced-docs/terraform/digital-ocean/#destroy","text":"To destroy: export DO_TOKEN = <place-api-token-here> terraform destroy -var \"do_token= ${ DO_TOKEN } \"","title":"Destroy"},{"location":"uk/advanced-docs/terraform/gcp/","text":"GCP \u00b6 Requirements \u00b6 GCP account Subscription on expressvpn.com (get the activation code) terraform installed Init \u00b6 To init Terraform run: terraform init Need to create terraform/gcp_expressvpn/terraform.tfvars file and set two variables values tou yours project_id = \"google-project-id\" expressvpn_key = \"expressvpn-activation-code\" Other vars can be overwritten in this file id needed. Deploy \u00b6 To deploy run: terraform apply Destroy \u00b6 To destroy infrastructure use commands: terraform destroy","title":"GCP"},{"location":"uk/advanced-docs/terraform/gcp/#gcp","text":"","title":"GCP"},{"location":"uk/advanced-docs/terraform/gcp/#requirements","text":"GCP account Subscription on expressvpn.com (get the activation code) terraform installed","title":"Requirements"},{"location":"uk/advanced-docs/terraform/gcp/#init","text":"To init Terraform run: terraform init Need to create terraform/gcp_expressvpn/terraform.tfvars file and set two variables values tou yours project_id = \"google-project-id\" expressvpn_key = \"expressvpn-activation-code\" Other vars can be overwritten in this file id needed.","title":"Init"},{"location":"uk/advanced-docs/terraform/gcp/#deploy","text":"To deploy run: terraform apply","title":"Deploy"},{"location":"uk/advanced-docs/terraform/gcp/#destroy","text":"To destroy infrastructure use commands: terraform destroy","title":"Destroy"},{"location":"uk/advanced-docs/terraform/heroku/","text":"Heroku \u00b6 Requirements \u00b6 Heroku account API token (Go to Account settings - API Key) and reveal API key terraform (1.0+) installed Deploy \u00b6 To deploy: export EMAIL = <place-email-here> export API_KEY = <place-api-key-here> terraform init terraform plan -var \"email= ${ EMAIL } \" -var \"api_key= ${ API_KEY } \" terraform apply -var \"email= ${ EMAIL } \" -var \"api_key= ${ API_KEY } \" Go to apps list and ensure that application successfully deployed. You can check logs for application with Heroku CLI: https://devcenter.heroku.com/articles/logging#view-logs Destroy \u00b6 To destroy: export EMAIL = <place-email-here> export API_KEY = <place-api-key-here> terraform destroy -var \"email= ${ EMAIL } \" -var \"api_key= ${ API_KEY } \"","title":"Heroku"},{"location":"uk/advanced-docs/terraform/heroku/#heroku","text":"","title":"Heroku"},{"location":"uk/advanced-docs/terraform/heroku/#requirements","text":"Heroku account API token (Go to Account settings - API Key) and reveal API key terraform (1.0+) installed","title":"Requirements"},{"location":"uk/advanced-docs/terraform/heroku/#deploy","text":"To deploy: export EMAIL = <place-email-here> export API_KEY = <place-api-key-here> terraform init terraform plan -var \"email= ${ EMAIL } \" -var \"api_key= ${ API_KEY } \" terraform apply -var \"email= ${ EMAIL } \" -var \"api_key= ${ API_KEY } \" Go to apps list and ensure that application successfully deployed. You can check logs for application with Heroku CLI: https://devcenter.heroku.com/articles/logging#view-logs","title":"Deploy"},{"location":"uk/advanced-docs/terraform/heroku/#destroy","text":"To destroy: export EMAIL = <place-email-here> export API_KEY = <place-api-key-here> terraform destroy -var \"email= ${ EMAIL } \" -var \"api_key= ${ API_KEY } \"","title":"Destroy"},{"location":"uk/advanced-docs/terraform/hetzner-cloud/","text":"Hetzner Cloud \u00b6 Requirements \u00b6 Hetzner Cloud account API token (Go to Project - Security - API Tokens) and create a token terraform (1.0+) installed Deploy \u00b6 To deploy: export HCLOUD_TOKEN = <place-api-token-here> export SSH_PUBLIC_KEY = \"<place-ssh-public-key-here>\" terraform init terraform plan -var \"hcloud_token= ${ HCLOUD_TOKEN } \" -var \"ssh_public_key= ${ SSH_PUBLIC_KEY } \" terraform apply -var \"hcloud_token= ${ HCLOUD_TOKEN } \" -var \"ssh_public_key= ${ SSH_PUBLIC_KEY } \" Destroy \u00b6 To destroy: export HCLOUD_TOKEN = <place-api-token-here> export SSH_PUBLIC_KEY = \"<place-ssh-public-key-here>\" terraform destroy -var \"hcloud_token= ${ HCLOUD_TOKEN } \" -var \"ssh_public_key= ${ SSH_PUBLIC_KEY } \"","title":"Hetzner Cloud"},{"location":"uk/advanced-docs/terraform/hetzner-cloud/#hetzner-cloud","text":"","title":"Hetzner Cloud"},{"location":"uk/advanced-docs/terraform/hetzner-cloud/#requirements","text":"Hetzner Cloud account API token (Go to Project - Security - API Tokens) and create a token terraform (1.0+) installed","title":"Requirements"},{"location":"uk/advanced-docs/terraform/hetzner-cloud/#deploy","text":"To deploy: export HCLOUD_TOKEN = <place-api-token-here> export SSH_PUBLIC_KEY = \"<place-ssh-public-key-here>\" terraform init terraform plan -var \"hcloud_token= ${ HCLOUD_TOKEN } \" -var \"ssh_public_key= ${ SSH_PUBLIC_KEY } \" terraform apply -var \"hcloud_token= ${ HCLOUD_TOKEN } \" -var \"ssh_public_key= ${ SSH_PUBLIC_KEY } \"","title":"Deploy"},{"location":"uk/advanced-docs/terraform/hetzner-cloud/#destroy","text":"To destroy: export HCLOUD_TOKEN = <place-api-token-here> export SSH_PUBLIC_KEY = \"<place-ssh-public-key-here>\" terraform destroy -var \"hcloud_token= ${ HCLOUD_TOKEN } \" -var \"ssh_public_key= ${ SSH_PUBLIC_KEY } \"","title":"Destroy"},{"location":"uk/advanced-docs/terraform/vultr/","text":"Vultr \u00b6 Requirements \u00b6 Vultr account API token terraform Deploy \u00b6 export VULTR_API_KEY = \"Your Vultr API Key\" terraform init terraform plan -var \"key=<path_to_ssh_key>\" -var \"num_inst=<number of instances to create>\" terraform apply -var \"key=<path_to_ssh_key>\" -var \"num_inst=<number of instances to create>\" Destroy \u00b6 To delete all the resources that were created run terraform destroy Tips \u00b6 Deploy script installs vnstat util that is useful for monitoring server network performance. Example, get network statistics for the last 5 hours: ssh root@ip vnstat -h 5","title":"Vultr"},{"location":"uk/advanced-docs/terraform/vultr/#vultr","text":"","title":"Vultr"},{"location":"uk/advanced-docs/terraform/vultr/#requirements","text":"Vultr account API token terraform","title":"Requirements"},{"location":"uk/advanced-docs/terraform/vultr/#deploy","text":"export VULTR_API_KEY = \"Your Vultr API Key\" terraform init terraform plan -var \"key=<path_to_ssh_key>\" -var \"num_inst=<number of instances to create>\" terraform apply -var \"key=<path_to_ssh_key>\" -var \"num_inst=<number of instances to create>\"","title":"Deploy"},{"location":"uk/advanced-docs/terraform/vultr/#destroy","text":"To delete all the resources that were created run terraform destroy","title":"Destroy"},{"location":"uk/advanced-docs/terraform/vultr/#tips","text":"Deploy script installs vnstat util that is useful for monitoring server network performance. Example, get network statistics for the last 5 hours: ssh root@ip vnstat -h 5","title":"Tips"}]}